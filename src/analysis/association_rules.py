"""An√°lise completa de regras de associa√ß√£o com Apriori, FP-Growth e Eclat"""

import logging
import pandas as pd
import numpy as np
from pathlib import Path
from collections import defaultdict
from itertools import combinations
from typing import List, Dict, Any, Tuple, Set
import warnings
warnings.filterwarnings('ignore')

# Check if mlxtend is available
try:
    from mlxtend.frequent_patterns import apriori, association_rules
    from mlxtend.preprocessing import TransactionEncoder
    MLXTEND_AVAILABLE = True
except ImportError:
    MLXTEND_AVAILABLE = False

class AssociationRulesAnalysis:
    """An√°lise completa de regras de associa√ß√£o com Apriori, FP-Growth e Eclat"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.apriori_results = {}
        self.fp_growth_results = {}
        self.eclat_results = {}
        
    def prepare_data(self, df: pd.DataFrame) -> List[List[str]]:
        """Preparar dados para an√°lise de regras de associa√ß√£o"""
        self.logger.info("üìä Preparando dados para regras de associa√ß√£o...")
        
        try:
            transactions = []
            
            # Converter dados para formato transacional
            categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
            
            # Remover target se existir
            if 'salary' in categorical_cols:
                categorical_cols.remove('salary')
            
            self.logger.info(f"üìã Colunas categ√≥ricas: {categorical_cols}")
            
            # Adicionar bins de vari√°veis num√©ricas
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if 'salary' in numeric_cols:
                numeric_cols.remove('salary')
            
            # Criar bins para vari√°veis num√©ricas
            df_processed = df.copy()
            
            for col in numeric_cols[:3]:  # Limitar a 3 para n√£o explodir o n√∫mero de itens
                try:
                    df_processed[f"{col}_bin"] = pd.cut(
                        df_processed[col], 
                        bins=3, 
                        labels=[f"{col}_low", f"{col}_medium", f"{col}_high"]
                    )
                    categorical_cols.append(f"{col}_bin")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao criar bins para {col}: {e}")
            
            # Criar transa√ß√µes
            for idx, row in df_processed.iterrows():
                transaction = []
                
                # Adicionar vari√°veis categ√≥ricas
                for col in categorical_cols:
                    if col in row and pd.notna(row[col]):
                        transaction.append(f"{col}_{row[col]}")
                
                # Adicionar target se existir
                if 'salary' in df.columns and pd.notna(row['salary']):
                    transaction.append(f"salary_{row['salary']}")
                
                # Adicionar apenas se temos items suficientes
                if len(transaction) >= 3:
                    transactions.append(transaction)
            
            self.logger.info(f"‚úÖ {len(transactions)} transa√ß√µes preparadas")
            return transactions
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na prepara√ß√£o dos dados: {e}")
            return []
        
        logging.info("üîß Preparando dados para an√°lise de associa√ß√£o...")
        
        # Discretizar vari√°veis num√©ricas
        df_discrete = df.copy()
        
        # Idade em faixas
        if 'age' in df.columns:
            df_discrete['age_group'] = pd.cut(df['age'], 
                                            bins=[0, 30, 40, 50, 100], 
                                            labels=['jovem', 'adulto', 'maduro', 'senior'])
        
        # Horas em faixas
        if 'hours-per-week' in df.columns:
            df_discrete['hours_group'] = pd.cut(df['hours-per-week'], 
                                              bins=[0, 35, 45, 100], 
                                              labels=['part_time', 'full_time', 'overtime'])
        
        # Educa√ß√£o em faixas
        if 'education-num' in df.columns:
            df_discrete['education_level'] = pd.cut(df['education-num'], 
                                                  bins=[0, 9, 12, 16, 20], 
                                                  labels=['basico', 'medio', 'superior', 'pos_grad'])
        
        # Criar transa√ß√µes
        transactions = []
        for _, row in df_discrete.iterrows():
            transaction = []
            
            # Adicionar caracter√≠sticas categ√≥ricas dispon√≠veis
            categorical_features = [
                ('workclass', 'workclass'),
                ('education', 'education'), 
                ('marital-status', 'marital'),
                ('occupation', 'occupation'),
                ('sex', 'sex'),
                ('age_group', 'age'),
                ('hours_group', 'hours'),
                ('education_level', 'edu_level'),
                ('salary', 'salary')
            ]
            
            for col, prefix in categorical_features:
                if col in row and pd.notna(row[col]):
                    transaction.append(f"{prefix}_{row[col]}")
            
            # Adicionar apenas se temos items suficientes
            if len(transaction) >= 3:
                transactions.append(transaction)
        
        logging.info(f"‚úÖ {len(transactions)} transa√ß√µes preparadas")
        return transactions
    
    def find_association_rules(self, transactions, min_support=0.03, min_confidence=0.5):
        """Encontrar regras de associa√ß√£o"""
        if not MLXTEND_AVAILABLE or not transactions:
            return pd.DataFrame()
        
        logging.info("üîç Buscando regras de associa√ß√£o...")
        
        try:
            # Codificar transa√ß√µes
            te = TransactionEncoder()
            te_ary = te.fit(transactions).transform(transactions)
            df_encoded = pd.DataFrame(te_ary, columns=te.columns_)
            
            logging.info(f"  üìä Dataset codificado: {df_encoded.shape}")
            
            # Encontrar itens frequentes
            self.frequent_itemsets = apriori(df_encoded, min_support=min_support, use_colnames=True)
            
            if self.frequent_itemsets.empty:
                logging.warning(f"‚ö†Ô∏è Nenhum itemset frequente encontrado com suporte >= {min_support}")
                return pd.DataFrame()
            
            logging.info(f"  üìä {len(self.frequent_itemsets)} itemsets frequentes encontrados")
            
            # Gerar regras
            self.rules = association_rules(self.frequent_itemsets, metric="confidence", min_threshold=min_confidence)
            
            if self.rules.empty:
                logging.warning(f"‚ö†Ô∏è Nenhuma regra encontrada com confian√ßa >= {min_confidence}")
                return pd.DataFrame()
            
            # Filtrar regras relacionadas com sal√°rio
            salary_rules = self.rules[
                self.rules['consequents'].astype(str).str.contains('salary_') |
                self.rules['antecedents'].astype(str).str.contains('salary_')
            ]
            
            logging.info(f"‚úÖ {len(self.rules)} regras totais, {len(salary_rules)} relacionadas a sal√°rio")
            
            # Salvar an√°lise
            self._save_rules_analysis(salary_rules)
            
            return salary_rules.sort_values('lift', ascending=False)
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro no Eclat: {e}")
            return {}

    def _eclat_recursive(self, current_itemsets: Dict, all_frequent: Dict, min_support: int, k: int):
        """Recurs√£o do Eclat para gerar itemsets de tamanho k"""
        if k > 3:  # Limitar profundidade para performance
            return
        
        new_itemsets = {}
        items = list(current_itemsets.keys())
        
        for i in range(len(items)):
            for j in range(i + 1, len(items)):
                # Intersec√ß√£o das listas de transa√ß√µes
                intersection = current_itemsets[items[i]] & current_itemsets[items[j]]
                
                if len(intersection) >= min_support:
                    # Criar novo itemset
                    new_itemset = f"{items[i]}+{items[j]}"
                    new_itemsets[new_itemset] = intersection
        
        if new_itemsets:
            all_frequent[k] = {frozenset(itemset.split('+')): len(tids) 
                              for itemset, tids in new_itemsets.items()}
            
            self.logger.info(f"   ‚Ä¢ L{k}: {len(new_itemsets)} itemsets frequentes de tamanho {k}")
            
            # Continuar recurs√£o
            self._eclat_recursive(new_itemsets, all_frequent, min_support, k + 1)

    def _generate_rules_eclat(self, frequent_itemsets: Dict[int, Dict], n_transactions: int, min_confidence: float = 0.6) -> List[Dict]:
        """Gerar regras para Eclat (mesmo m√©todo do Apriori)"""
        return self._generate_rules_apriori(frequent_itemsets, n_transactions, min_confidence)

    def compare_algorithms(self) -> Dict[str, Any]:
        """Comparar resultados dos tr√™s algoritmos"""
        comparison = {
            'algorithms': ['Apriori', 'FP-Growth', 'Eclat'],
            'results': []
        }
        
        for alg_name, results in [
            ('Apriori', self.apriori_results),
            ('FP-Growth', self.fp_growth_results),
            ('Eclat', self.eclat_results)
        ]:
            if results and results.get('rules'):
                rules = results['rules']
                comparison['results'].append({
                    'Algorithm': alg_name,
                    'Rules_Found': len(rules),
                    'Avg_Confidence': np.mean([r['confidence'] for r in rules]),
                    'Avg_Lift': np.mean([r['lift'] for r in rules]),
                    'Max_Confidence': max([r['confidence'] for r in rules]),
                    'Execution_Status': 'Success'
                })
            else:
                comparison['results'].append({
                    'Algorithm': alg_name,
                    'Rules_Found': 0,
                    'Avg_Confidence': 0,
                    'Avg_Lift': 0,
                    'Max_Confidence': 0,
                    'Execution_Status': 'Failed'
                })
        
        return comparison

    def save_results(self, output_dir: str = "output/analysis"):
        """Salvar resultados das an√°lises"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Salvar resultados de cada algoritmo
        for alg_name, results in [
            ('apriori', self.apriori_results),
            ('fp_growth', self.fp_growth_results),
            ('eclat', self.eclat_results)
        ]:
            if results and results.get('rules'):
                rules_df = pd.DataFrame(results['rules'])
                rules_df.to_csv(output_path / f"{alg_name}_rules.csv", index=False)
                
                self.logger.info(f"üíæ {alg_name.upper()} salvo: {len(results['rules'])} regras")
        
        # Salvar compara√ß√£o
        comparison = self.compare_algorithms()
        if comparison.get('results'):
            comparison_df = pd.DataFrame(comparison['results'])
            comparison_df.to_csv(output_path / "association_algorithms_comparison.csv", index=False)
            
            self.logger.info("üíæ Compara√ß√£o salva: association_algorithms_comparison.csv")

    def run_complete_analysis(self, df: pd.DataFrame, min_support: float = 0.01, min_confidence: float = 0.6) -> Dict[str, Any]:
        """Executar an√°lise completa com todos os algoritmos"""
        self.logger.info("üöÄ Iniciando an√°lise completa de regras de associa√ß√£o...")
        
        # Preparar dados
        transactions = self.prepare_data(df)
        
        if not transactions:
            self.logger.error("‚ùå Nenhuma transa√ß√£o v√°lida para an√°lise")
            return {}
        
        results = {}
        
        # Executar algoritmos
        results['apriori'] = self.run_apriori(transactions, min_support, min_confidence)
        results['fp_growth'] = self.run_fp_growth(transactions, min_support, min_confidence)
        results['eclat'] = self.run_eclat(transactions, min_support, min_confidence)
        
        # Compara√ß√£o
        results['comparison'] = self.compare_algorithms()
        
        # Salvar resultados
        self.save_results()
        
        return results
    
    def run_apriori(self, transactions: List[List[str]], min_support: float = 0.01, min_confidence: float = 0.6) -> Dict[str, Any]:
        """Executar algoritmo Apriori"""
        self.logger.info("üîç Executando APRIORI...")
        
        try:
            if not MLXTEND_AVAILABLE:
                self.logger.warning("‚ö†Ô∏è MLxtend n√£o dispon√≠vel - usando implementa√ß√£o b√°sica")
                return self._run_apriori_basic(transactions, min_support, min_confidence)
            
            # Usar MLxtend se dispon√≠vel
            from mlxtend.preprocessing import TransactionEncoder
            from mlxtend.frequent_patterns import apriori, association_rules
            
            # Codificar transa√ß√µes
            te = TransactionEncoder()
            te_ary = te.fit(transactions).transform(transactions)
            df_encoded = pd.DataFrame(te_ary, columns=te.columns_)
            
            # Encontrar itemsets frequentes
            frequent_itemsets = apriori(df_encoded, min_support=min_support, use_colnames=True)
            
            if frequent_itemsets.empty:
                self.logger.warning(f"‚ö†Ô∏è APRIORI: Nenhum itemset frequente (suporte >= {min_support})")
                return {'rules': [], 'frequent_itemsets': [], 'status': 'no_patterns'}
            
            # Gerar regras
            rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=min_confidence)
            
            if rules.empty:
                self.logger.warning(f"‚ö†Ô∏è APRIORI: Nenhuma regra (confian√ßa >= {min_confidence})")
                return {'rules': [], 'frequent_itemsets': len(frequent_itemsets), 'status': 'no_rules'}
            
            # Converter para formato padr√£o
            rules_list = []
            for _, rule in rules.iterrows():
                rules_list.append({
                    'antecedents': list(rule['antecedents']),
                    'consequents': list(rule['consequents']),
                    'support': float(rule['support']),
                    'confidence': float(rule['confidence']),
                    'lift': float(rule['lift']),
                    'conviction': float(rule.get('conviction', 1.0))
                })
            
            result = {
                'rules': rules_list,
                'frequent_itemsets': len(frequent_itemsets),
                'total_rules': len(rules_list),
                'status': 'success'
            }
            
            self.apriori_results = result
            self.logger.info(f"‚úÖ APRIORI: {len(rules_list)} regras encontradas")
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro no APRIORI: {e}")
            return {'rules': [], 'status': 'error', 'error': str(e)}
    
    def run_fp_growth(self, transactions: List[List[str]], min_support: float = 0.01, min_confidence: float = 0.6) -> Dict[str, Any]:
        """Executar algoritmo FP-Growth"""
        self.logger.info("üå≥ Executando FP-GROWTH...")
        
        try:
            if not MLXTEND_AVAILABLE:
                self.logger.warning("‚ö†Ô∏è MLxtend n√£o dispon√≠vel - usando implementa√ß√£o b√°sica")
                return self._run_fp_growth_basic(transactions, min_support, min_confidence)
            
            # Usar MLxtend se dispon√≠vel
            from mlxtend.preprocessing import TransactionEncoder
            from mlxtend.frequent_patterns import fpgrowth, association_rules
            
            # Codificar transa√ß√µes
            te = TransactionEncoder()
            te_ary = te.fit(transactions).transform(transactions)
            df_encoded = pd.DataFrame(te_ary, columns=te.columns_)
            
            # Encontrar itemsets frequentes com FP-Growth
            frequent_itemsets = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)
            
            if frequent_itemsets.empty:
                self.logger.warning(f"‚ö†Ô∏è FP-GROWTH: Nenhum itemset frequente (suporte >= {min_support})")
                return {'rules': [], 'frequent_itemsets': [], 'status': 'no_patterns'}
            
            # Gerar regras
            rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=min_confidence)
            
            if rules.empty:
                self.logger.warning(f"‚ö†Ô∏è FP-GROWTH: Nenhuma regra (confian√ßa >= {min_confidence})")
                return {'rules': [], 'frequent_itemsets': len(frequent_itemsets), 'status': 'no_rules'}
            
            # Converter para formato padr√£o
            rules_list = []
            for _, rule in rules.iterrows():
                rules_list.append({
                    'antecedents': list(rule['antecedents']),
                    'consequents': list(rule['consequents']),
                    'support': float(rule['support']),
                    'confidence': float(rule['confidence']),
                    'lift': float(rule['lift']),
                    'conviction': float(rule.get('conviction', 1.0))
                })
            
            result = {
                'rules': rules_list,
                'frequent_itemsets': len(frequent_itemsets),
                'total_rules': len(rules_list),
                'status': 'success'
            }
            
            self.fp_growth_results = result
            self.logger.info(f"‚úÖ FP-GROWTH: {len(rules_list)} regras encontradas")
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro no FP-GROWTH: {e}")
            return {'rules': [], 'status': 'error', 'error': str(e)}
    
    def run_eclat(self, transactions: List[List[str]], min_support: float = 0.01, min_confidence: float = 0.6) -> Dict[str, Any]:
        """Executar algoritmo Eclat"""
        self.logger.info("‚ö° Executando ECLAT...")
        
        try:
            # Implementa√ß√£o b√°sica do ECLAT
            min_support_count = int(min_support * len(transactions))
            
            # Encontrar items √∫nicos e suas transa√ß√µes
            item_transactions = defaultdict(set)
            for tid, transaction in enumerate(transactions):
                for item in transaction:
                    item_transactions[item].add(tid)
            
            # Filtrar items frequentes
            frequent_items = {
                item: tids for item, tids in item_transactions.items()
                if len(tids) >= min_support_count
            }
            
            if not frequent_items:
                self.logger.warning(f"‚ö†Ô∏è ECLAT: Nenhum item frequente (suporte >= {min_support})")
                return {'rules': [], 'frequent_itemsets': 0, 'status': 'no_patterns'}
            
            # Encontrar itemsets frequentes de tamanho 2
            frequent_itemsets = {1: {frozenset([item]): len(tids) for item, tids in frequent_items.items()}}
            
            self._eclat_recursive(frequent_items, frequent_itemsets, min_support_count, 2)
            
            # Gerar regras
            rules = self._generate_rules_eclat(frequent_itemsets, len(transactions), min_confidence)
            
            if not rules:
                self.logger.warning(f"‚ö†Ô∏è ECLAT: Nenhuma regra (confian√ßa >= {min_confidence})")
                return {'rules': [], 'frequent_itemsets': sum(len(fs) for fs in frequent_itemsets.values()), 'status': 'no_rules'}
            
            result = {
                'rules': rules,
                'frequent_itemsets': sum(len(fs) for fs in frequent_itemsets.values()),
                'total_rules': len(rules),
                'status': 'success'
            }
            
            self.eclat_results = result
            self.logger.info(f"‚úÖ ECLAT: {len(rules)} regras encontradas")
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro no ECLAT: {e}")
            return {'rules': [], 'status': 'error', 'error': str(e)}
    
    def _run_apriori_basic(self, transactions: List[List[str]], min_support: float, min_confidence: float) -> Dict[str, Any]:
        """Implementa√ß√£o b√°sica do Apriori quando MLxtend n√£o est√° dispon√≠vel"""
        try:
            min_support_count = int(min_support * len(transactions))
            
            # Contar items individuais
            item_counts = defaultdict(int)
            for transaction in transactions:
                for item in transaction:
                    item_counts[item] += 1
            
            # Items frequentes de tamanho 1
            frequent_1 = {
                frozenset([item]): count for item, count in item_counts.items()
                if count >= min_support_count
            }
            
            if not frequent_1:
                return {'rules': [], 'status': 'no_patterns'}
            
            # Gerar itemsets de tamanho 2
            frequent_2 = {}
            items = list(frequent_1.keys())
            
            for i in range(len(items)):
                for j in range(i + 1, len(items)):
                    candidate = items[i] | items[j]
                    count = sum(1 for t in transactions if candidate.issubset(set(t)))
                    
                    if count >= min_support_count:
                        frequent_2[candidate] = count
            
            frequent_itemsets = {1: frequent_1, 2: frequent_2}
            
            # Gerar regras
            rules = self._generate_rules_apriori(frequent_itemsets, len(transactions), min_confidence)
            
            return {
                'rules': rules,
                'frequent_itemsets': len(frequent_1) + len(frequent_2),
                'total_rules': len(rules),
                'status': 'success'
            }
            
        except Exception as e:
            return {'rules': [], 'status': 'error', 'error': str(e)}
    
    def _run_fp_growth_basic(self, transactions: List[List[str]], min_support: float, min_confidence: float) -> Dict[str, Any]:
        """Implementa√ß√£o b√°sica do FP-Growth quando MLxtend n√£o est√° dispon√≠vel"""
        # Por simplicidade, usar a mesma implementa√ß√£o