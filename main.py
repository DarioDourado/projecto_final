#!/usr/bin/env python3
"""
üöÄ Pipeline Principal - Sistema H√≠brido SQL‚ÜíCSV
Otimizado para Streamlit Community Cloud com fallback autom√°tico
"""

import os
import sys
import logging
import argparse
import traceback
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import pandas as pd
import numpy as np

# Adicionar src ao path
sys.path.append(str(Path(__file__).parent / "src"))

def setup_logging():
    """Configurar sistema de logging otimizado"""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"pipeline_{timestamp}.log"
    
    log_format = '%(asctime)s | %(levelname)-8s | %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        datefmt=date_format,
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger("HybridPipeline")
    logger.info("üöÄ PIPELINE H√çBRIDO SQL‚ÜíCSV INICIADO")
    logger.info(f"üìù Log salvo em: {log_file}")
    return logger

class HybridPipelineSQL:
    """
    Pipeline H√≠brido com Fallback Autom√°tico SQL‚ÜíCSV
    Otimizado para Streamlit Community Cloud
    """
    
    def __init__(self, force_csv=False, log_level="INFO"):
        """Inicializar pipeline h√≠brido"""
        self.logger = setup_logging()
        self.logger.setLevel(getattr(logging, log_level.upper()))
        
        self.force_csv = force_csv
        self.data_source = None  # 'sql' ou 'csv'
        self.results = {}
        self.models = {}
        self.df = None
        self.start_time = datetime.now()
        
        # M√©tricas de performance
        self.performance_metrics = {
            'start_time': self.start_time,
            'data_source': None,
            'data_load_time': None,
            'ml_training_time': None,
            'total_time': None,
            'records_processed': 0
        }
        
        self.logger.info(f"üîß Pipeline h√≠brido inicializado:")
        self.logger.info(f"   ‚Ä¢ For√ßar CSV: {force_csv}")
        self.logger.info(f"   ‚Ä¢ N√≠vel de log: {log_level}")
        
        self._initialize_components()

    def _initialize_components(self):
        """Inicializar componentes com fallback"""
        try:
            # 1. Tentar pipeline SQL primeiro (se n√£o for√ßar CSV)
            if not self.force_csv:
                try:
                    # ‚úÖ CORRE√á√ÉO: Import correto
                    from src.pipelines.data_pipeline import DataPipelineSQL
                    self.sql_pipeline = DataPipelineSQL(force_migration=False)
                    self.logger.info("‚úÖ Pipeline SQL dispon√≠vel")
                except ImportError as e:
                    self.logger.warning(f"‚ö†Ô∏è Pipeline SQL indispon√≠vel: {e}")
                    self.sql_pipeline = None
            else:
                self.sql_pipeline = None
                self.logger.info("üìã Modo CSV for√ßado")
            
            # 2. Pipeline ML (sempre dispon√≠vel)
            try:
                from src.pipelines.ml_pipeline import MLPipeline
                self.ml_pipeline = MLPipeline()
                self.logger.info("‚úÖ Pipeline ML inicializado")
            except ImportError as e:
                self.logger.error(f"‚ùå Pipeline ML n√£o dispon√≠vel: {e}")
                self.ml_pipeline = None
            
            # 3. Pipelines opcionais
            self._initialize_optional_pipelines()
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}")

    def _initialize_optional_pipelines(self):
        """Inicializar pipelines opcionais"""
        # Clustering
        try:
            from src.pipelines.clustering_pipeline import ClusteringPipeline
            self.clustering_pipeline = ClusteringPipeline()
            self.logger.info("‚úÖ Pipeline Clustering dispon√≠vel")
        except ImportError:
            self.clustering_pipeline = None
            self.logger.info("‚ÑπÔ∏è Pipeline Clustering n√£o dispon√≠vel")
        
        # Association Rules
        try:
            from src.pipelines.association_pipeline import AssociationPipeline
            self.association_pipeline = AssociationPipeline()
            self.logger.info("‚úÖ Pipeline Association dispon√≠vel")
        except ImportError:
            self.association_pipeline = None
            self.logger.info("‚ÑπÔ∏è Pipeline Association n√£o dispon√≠vel")

    def run(self) -> Dict[str, Any]:
        """Executar pipeline completo com fallback autom√°tico"""
        try:
            self.logger.info("üöÄ INICIANDO PIPELINE H√çBRIDO")
            self.logger.info("=" * 60)
            
            # 1. Carregar dados (SQL ‚Üí CSV fallback)
            self._run_data_pipeline()
            
            if self.df is None or len(self.df) == 0:
                raise ValueError("‚ùå Nenhum dado foi carregado")
            
            # 2. Machine Learning
            self._run_ml_pipeline()
            
            # 3. An√°lises opcionais
            self._run_optional_analysis()
            
            # 4. Finalizar
            self._finalize_pipeline()
            
            return self._prepare_results()
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro cr√≠tico no pipeline: {e}")
            self.logger.error(traceback.format_exc())
            return {'error': str(e), 'data_source': self.data_source}

    def _run_data_pipeline(self):
        """Executar carregamento de dados com fallback SQL‚ÜíCSV"""
        self.logger.info("üìä CARREGAMENTO DE DADOS")
        self.logger.info("-" * 40)
        
        data_start = datetime.now()
        
        # Tentar SQL primeiro (se dispon√≠vel)
        if self.sql_pipeline and not self.force_csv:
            self.logger.info("üóÑÔ∏è Tentando carregar dados do SQL...")
            try:
                self.df = self.sql_pipeline.run()
                if self.df is not None and len(self.df) > 0:
                    self.data_source = 'sql'
                    self.logger.info(f"‚úÖ Dados carregados do SQL: {len(self.df):,} registros")
                    self._log_data_details()
                else:
                    self.logger.warning("‚ö†Ô∏è SQL retornou dados vazios, tentando CSV...")
                    self._load_from_csv()
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro no SQL: {e}")
                self.logger.info("üîÑ Fazendo fallback para CSV...")
                self._load_from_csv()
        else:
            # Carregar diretamente do CSV
            self._load_from_csv()
        
        # Calcular tempo de carregamento
        data_time = datetime.now() - data_start
        self.performance_metrics['data_load_time'] = data_time.total_seconds()
        self.performance_metrics['data_source'] = self.data_source
        self.performance_metrics['records_processed'] = len(self.df) if self.df is not None else 0
        
        self.logger.info(f"‚è±Ô∏è Tempo de carregamento: {data_time.total_seconds():.2f}s")

    def _load_from_csv(self):
        """Carregar dados do arquivo CSV"""
        csv_paths = [
            "data/raw/4-Carateristicas_salario.csv",
            "4-Carateristicas_salario.csv",
            "bkp/4-Carateristicas_salario.csv",
            "data/4-Carateristicas_salario.csv"
        ]
        
        for csv_path in csv_paths:
            if Path(csv_path).exists():
                try:
                    self.logger.info(f"üìã Carregando CSV: {csv_path}")
                    self.df = pd.read_csv(csv_path)
                    
                    if len(self.df) > 0:
                        self.data_source = 'csv'
                        self.logger.info(f"‚úÖ CSV carregado: {len(self.df):,} registros, {len(self.df.columns)} colunas")
                        
                        # Limpeza b√°sica
                        self._basic_cleaning()
                        self._log_data_details()
                        return
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Erro ao carregar {csv_path}: {e}")
        
        # Se chegou aqui, n√£o encontrou nenhum CSV
        self.logger.error("‚ùå Nenhum arquivo CSV encontrado!")
        self.logger.info("üí° Locais procurados:")
        for path in csv_paths:
            self.logger.info(f"   ‚Ä¢ {path}")

    def _basic_cleaning(self):
        """Limpeza b√°sica dos dados CSV"""
        if self.df is None:
            return
        
        initial_size = len(self.df)
        
        # Remover espa√ßos e caracteres especiais
        for col in self.df.select_dtypes(include=['object']).columns:
            self.df[col] = self.df[col].astype(str).str.strip()
        
        # Substituir '?' por NaN
        self.df = self.df.replace('?', np.nan)
        
        # Remover linhas completamente vazias
        self.df = self.df.dropna(how='all')
        
        final_size = len(self.df)
        if final_size != initial_size:
            self.logger.info(f"üßπ Limpeza: {initial_size:,} ‚Üí {final_size:,} registros")

    def _log_data_details(self):
        """Log detalhado dos dados carregados"""
        if self.df is None:
            return
        
        self.logger.info("üìà DETALHES DOS DADOS:")
        self.logger.info(f"   üìã Registros: {len(self.df):,}")
        self.logger.info(f"   üìä Colunas: {len(self.df.columns)}")
        self.logger.info(f"   üíæ Mem√≥ria: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # Colunas por tipo
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        categorical_cols = self.df.select_dtypes(include=['object']).columns
        
        self.logger.info(f"   üî¢ Num√©ricas: {len(numeric_cols)}")
        self.logger.info(f"   üìù Categ√≥ricas: {len(categorical_cols)}")
        
        # Dados ausentes
        missing_data = self.df.isnull().sum()
        if missing_data.sum() > 0:
            self.logger.warning("‚ö†Ô∏è DADOS AUSENTES:")
            for col, missing_count in missing_data[missing_data > 0].items():
                percentage = (missing_count / len(self.df)) * 100
                self.logger.warning(f"   ‚Ä¢ {col}: {missing_count} ({percentage:.1f}%)")
        else:
            self.logger.info("‚úÖ Sem dados ausentes")
        
        # Duplicatas
        duplicates = self.df.duplicated().sum()
        if duplicates > 0:
            self.logger.warning(f"‚ö†Ô∏è {duplicates} duplicatas ({duplicates/len(self.df)*100:.1f}%)")
        else:
            self.logger.info("‚úÖ Sem duplicatas")
        
        # Distribui√ß√£o target (se existir)
        if 'salary' in self.df.columns:
            target_dist = self.df['salary'].value_counts()
            self.logger.info("üéØ DISTRIBUI√á√ÉO TARGET:")
            for value, count in target_dist.items():
                percentage = (count / len(self.df)) * 100
                self.logger.info(f"   ‚Ä¢ {value}: {count:,} ({percentage:.1f}%)")

    def _run_ml_pipeline(self):
        """Executar pipeline de ML"""
        if not self.ml_pipeline:
            self.logger.warning("‚ö†Ô∏è Pipeline ML n√£o dispon√≠vel")
            return
        
        self.logger.info("ü§ñ MACHINE LEARNING")
        self.logger.info("-" * 40)
        
        ml_start = datetime.now()
        
        try:
            self.models, ml_results = self.ml_pipeline.run(self.df)
            
            if self.models:
                self.logger.info(f"‚úÖ {len(self.models)} modelos treinados")
                
                # Log performance de cada modelo
                for name, model_info in self.models.items():
                    if isinstance(model_info, dict) and 'accuracy' in model_info:
                        accuracy = model_info['accuracy']
                        self.logger.info(f"   ‚Ä¢ {name}: {accuracy:.4f}")
                        
                        # Classifica√ß√£o de performance
                        if accuracy > 0.90:
                            self.logger.info(f"     üèÜ EXCELENTE")
                        elif accuracy > 0.85:
                            self.logger.info(f"     ‚úÖ MUITO BOA")
                        elif accuracy > 0.80:
                            self.logger.info(f"     ‚ö†Ô∏è BOA")
                        else:
                            self.logger.warning(f"     ‚ùå REGULAR")
                
                # Identificar melhor modelo
                best_model, best_score = self._find_best_model()
                if best_model:
                    self.logger.info(f"üèÜ MELHOR MODELO: {best_model} ({best_score:.4f})")
                
                self.results['ml_results'] = ml_results
            else:
                self.logger.warning("‚ö†Ô∏è Nenhum modelo foi treinado")
        
        except Exception as e:
            self.logger.error(f"‚ùå Erro no ML: {e}")
            self.logger.error(traceback.format_exc())
        
        ml_time = datetime.now() - ml_start
        self.performance_metrics['ml_training_time'] = ml_time.total_seconds()
        self.logger.info(f"‚è±Ô∏è Tempo ML: {ml_time.total_seconds():.2f}s")

    def _find_best_model(self) -> Tuple[Optional[str], float]:
        """Encontrar melhor modelo"""
        best_model = None
        best_score = 0.0
        
        for model_name, model_info in self.models.items():
            if isinstance(model_info, dict) and 'accuracy' in model_info:
                accuracy = model_info['accuracy']
                if accuracy > best_score:
                    best_score = accuracy
                    best_model = model_name
        
        return best_model, best_score

    def _run_optional_analysis(self):
        """Executar an√°lises opcionais"""
        self.logger.info("üìä AN√ÅLISES OPCIONAIS")
        self.logger.info("-" * 40)
        
        # Clustering
        if self.clustering_pipeline:
            try:
                self.logger.info("üéØ Executando clustering...")
                clustering_results = self.clustering_pipeline.run(self.df)
                if clustering_results:
                    self.results['clustering'] = clustering_results
                    self.logger.info("‚úÖ Clustering conclu√≠do")
                    
                    # Log resultados de clustering
                    for algorithm, result in clustering_results.items():
                        if isinstance(result, dict):
                            if 'n_clusters' in result:
                                self.logger.info(f"   ‚Ä¢ {algorithm}: {result['n_clusters']} clusters")
                            if 'silhouette_score' in result:
                                score = result['silhouette_score']
                                self.logger.info(f"     Silhouette: {score:.4f}")
                else:
                    self.logger.warning("‚ö†Ô∏è Clustering n√£o retornou resultados")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro no clustering: {e}")
        else:
            self.logger.info("‚ÑπÔ∏è Clustering n√£o dispon√≠vel")
        
        # Association Rules
        if self.association_pipeline:
            try:
                self.logger.info("üìã Executando regras de associa√ß√£o...")
                association_results = self.association_pipeline.run(self.df)
                if association_results:
                    self.results['association_rules'] = association_results
                    self.logger.info("‚úÖ Regras de associa√ß√£o conclu√≠das")
                    
                    # Log resultados
                    if 'rules' in association_results:
                        rules_count = len(association_results['rules'])
                        self.logger.info(f"   ‚Ä¢ {rules_count} regras encontradas")
                else:
                    self.logger.warning("‚ö†Ô∏è Regras de associa√ß√£o n√£o retornaram resultados")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro nas regras de associa√ß√£o: {e}")
        else:
            self.logger.info("‚ÑπÔ∏è Regras de associa√ß√£o n√£o dispon√≠veis")

    def _finalize_pipeline(self):
        """Finalizar pipeline"""
        total_time = datetime.now() - self.start_time
        self.performance_metrics['total_time'] = total_time.total_seconds()
        
        self.logger.info("üéâ PIPELINE CONCLU√çDO")
        self.logger.info("=" * 60)
        self.logger.info(f"‚è±Ô∏è Tempo total: {total_time.total_seconds():.2f}s")
        self.logger.info(f"üìä Fonte dos dados: {self.data_source.upper()}")
        self.logger.info(f"üìã Registros processados: {self.performance_metrics['records_processed']:,}")
        
        if self.models:
            self.logger.info(f"ü§ñ Modelos treinados: {len(self.models)}")
            best_model, best_score = self._find_best_model()
            if best_model:
                self.logger.info(f"üèÜ Melhor performance: {best_model} ({best_score:.4f})")
        
        # Salvar resultados
        self._save_results()

    def _save_results(self):
        """Salvar resultados do pipeline"""
        try:
            output_dir = Path("output")
            output_dir.mkdir(exist_ok=True)
            
            # Salvar estado do pipeline
            pipeline_state = {
                'data_source': self.data_source,
                'models_count': len(self.models),
                'performance_metrics': self.performance_metrics,
                'results_summary': {
                    'has_ml': len(self.models) > 0,
                    'has_clustering': 'clustering' in self.results,
                    'has_association': 'association_rules' in self.results
                },
                'timestamp': datetime.now().isoformat()
            }
            
            # Salvar em JSON
            state_file = output_dir / "pipeline_state.json"
            with open(state_file, 'w', encoding='utf-8') as f:
                json.dump(pipeline_state, f, indent=2, default=str, ensure_ascii=False)
            
            self.logger.info(f"üíæ Estado salvo: {state_file}")
            
            # Salvar modelos se existirem
            if self.models:
                models_file = output_dir / "models_summary.json"
                models_summary = {}
                
                for name, model_info in self.models.items():
                    if isinstance(model_info, dict):
                        models_summary[name] = {
                            k: v for k, v in model_info.items() 
                            if isinstance(v, (int, float, str, bool))
                        }
                
                with open(models_file, 'w', encoding='utf-8') as f:
                    json.dump(models_summary, f, indent=2, default=str, ensure_ascii=False)
                
                self.logger.info(f"üíæ Modelos salvos: {models_file}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao salvar resultados: {e}")

    def _prepare_results(self) -> Dict[str, Any]:
        """Preparar resultados finais"""
        return {
            'df': self.df,
            'models': self.models,
            'results': self.results,
            'data_source': self.data_source,
            'performance_metrics': self.performance_metrics,
            'status': self._generate_status_message()
        }

    def _generate_status_message(self) -> str:
        """Gerar mensagem de status final"""
        if self.df is not None and len(self.df) > 0:
            if self.models and len(self.models) > 0:
                best_model, best_score = self._find_best_model()
                return f"‚úÖ Pipeline conclu√≠do - Fonte: {self.data_source.upper()} | Melhor modelo: {best_model} ({best_score:.4f})"
            else:
                return f"‚ö†Ô∏è Dados carregados via {self.data_source.upper()}, mas problemas no ML"
        else:
            return "‚ùå Falha no carregamento de dados"

def setup_database():
    """Configurar banco de dados"""
    print("üóÑÔ∏è Configura√ß√£o de Banco de Dados")
    print("=" * 40)
    
    try:
        from src.database.setup import setup_database as setup_db
        setup_db()
    except ImportError:
        print("‚ö†Ô∏è M√≥dulos de banco n√£o encontrados")
        print("üí° Sistema funcionar√° em modo CSV")
        print("\nPara habilitar SQL:")
        print("  1. pip install mysql-connector-python")
        print("  2. Configure vari√°veis de ambiente:")
        print("     export DB_HOST=localhost")
        print("     export DB_NAME=salary_analysis") 
        print("     export DB_USER=salary_user")
        print("     export DB_PASSWORD=senha_forte")

def main():
    """Fun√ß√£o principal com argumentos otimizados"""
    parser = argparse.ArgumentParser(description='Pipeline H√≠brido SQL‚ÜíCSV')
    parser.add_argument('--csv-only', action='store_true', help='For√ßar uso apenas de CSV')
    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    parser.add_argument('--setup-db', action='store_true', help='Configurar banco de dados')
    
    args = parser.parse_args()
    
    try:
        if args.setup_db:
            setup_database()
            return
        
        # Executar pipeline principal
        pipeline = HybridPipelineSQL(
            force_csv=args.csv_only,
            log_level=args.log_level
        )
        
        results = pipeline.run()
        
        if 'error' not in results:
            print(f"\nüéâ PIPELINE CONCLU√çDO COM SUCESSO!")
            print(f"üìä Fonte: {results['data_source'].upper()}")
            print(f"üìã Registros: {len(results['df']):,}")
            print(f"ü§ñ Modelos: {len(results['models'])}")
            print(f"‚è±Ô∏è Tempo: {results['performance_metrics']['total_time']:.2f}s")
            print(f"\nüí° Pr√≥ximo passo: streamlit run app.py")
        else:
            print(f"\n‚ùå ERRO NO PIPELINE: {results['error']}")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Pipeline interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro cr√≠tico: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()