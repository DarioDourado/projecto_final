"""
üìà Testes de Performance e M√©tricas
Valida√ß√£o detalhada de todas as m√©tricas de Data Science
"""

import pytest
import pandas as pd
import numpy as np
import json
from pathlib import Path
import sys
from datetime import datetime

sys.path.append(str(Path(__file__).parent.parent / "src"))

class TestPerformanceMetrics:
    """Testes de m√©tricas de performance"""
    
    def test_pipeline_execution_time(self):
        """Testar tempo de execu√ß√£o do pipeline"""
        try:
            from main import HybridPipelineSQL
            
            start_time = datetime.now()
            
            pipeline = HybridPipelineSQL(force_csv=True, log_level="WARNING")
            results = pipeline.run()
            
            execution_time = datetime.now() - start_time
            
            # Validar que executou em tempo razo√°vel (< 30 segundos)
            assert execution_time.total_seconds() < 30, f"Pipeline muito lento: {execution_time.total_seconds():.2f}s"
            
            # Validar m√©tricas de performance
            if 'performance_metrics' in results:
                metrics = results['performance_metrics']
                
                assert 'total_time' in metrics
                assert 'data_load_time' in metrics
                assert 'ml_training_time' in metrics
                
                print(f"‚è±Ô∏è M√âTRICAS DE PERFORMANCE:")
                print(f"   üìä Tempo total: {metrics['total_time']:.2f}s")
                print(f"   üìã Carregamento: {metrics['data_load_time']:.2f}s")
                print(f"   ü§ñ ML Training: {metrics['ml_training_time']:.2f}s")
                print(f"   üìà Registros: {metrics['records_processed']:,}")
            
        except ImportError:
            pytest.skip("Pipeline principal n√£o dispon√≠vel")
    
    def test_data_science_results_validation(self):
        """Validar todos os resultados importantes de Data Science"""
        try:
            from main import HybridPipelineSQL
            
            pipeline = HybridPipelineSQL(force_csv=True, log_level="WARNING")
            results = pipeline.run()
            
            # Validar estrutura de resultados
            required_keys = ['df', 'models', 'results', 'data_source', 'performance_metrics', 'status']
            
            for key in required_keys:
                assert key in results, f"Chave ausente nos resultados: {key}"
            
            # Validar dados
            df = results['df']
            assert df is not None and len(df) > 0, "Dataset vazio nos resultados"
            
            # Validar modelos
            models = results['models']
            if len(models) > 0:
                print(f"ü§ñ MODELOS TREINADOS: {len(models)}")
                
                for name, model in models.items():
                    print(f"   ‚úÖ {name}")
                    
                    # Validar que √© um modelo v√°lido
                    assert hasattr(model, 'predict') or isinstance(model, dict), f"Modelo inv√°lido: {name}"
            
            # Validar m√©tricas de qualidade dos dados
            self._validate_data_quality(df)
            
            # Validar fonte de dados
            data_source = results['data_source']
            assert data_source in ['sql', 'csv'], f"Fonte de dados inv√°lida: {data_source}"
            
            print(f"‚úÖ VALIDA√á√ÉO COMPLETA DOS RESULTADOS:")
            print(f"   üìä Fonte: {data_source.upper()}")
            print(f"   üìã Registros: {len(df):,}")
            print(f"   ü§ñ Modelos: {len(models)}")
            print(f"   üìà Status: {results['status']}")
            
        except ImportError:
            pytest.skip("Pipeline principal n√£o dispon√≠vel")
    
    def _validate_data_quality(self, df):
        """Validar qualidade dos dados nos resultados"""
        # M√©tricas b√°sicas
        total_records = len(df)
        total_columns = len(df.columns)
        
        print(f"üìä QUALIDADE DOS DADOS:")
        print(f"   üìã Registros: {total_records:,}")
        print(f"   üìä Colunas: {total_columns}")
        
        # Tipos de dados
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=['object']).columns
        
        print(f"   üî¢ Num√©ricas: {len(numeric_cols)}")
        print(f"   üìù Categ√≥ricas: {len(categorical_cols)}")
        
        # Dados ausentes
        missing_data = df.isnull().sum()
        cols_with_missing = missing_data[missing_data > 0]
        
        if len(cols_with_missing) > 0:
            print(f"   ‚ö†Ô∏è Colunas com missing:")
            for col, count in cols_with_missing.items():
                percentage = (count / total_records) * 100
                print(f"     ‚Ä¢ {col}: {count} ({percentage:.1f}%)")
        else:
            print(f"   ‚úÖ Sem dados ausentes")
        
        # Duplicatas
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            dup_percentage = (duplicates / total_records) * 100
            print(f"   ‚ö†Ô∏è Duplicatas: {duplicates} ({dup_percentage:.1f}%)")
        else:
            print(f"   ‚úÖ Sem duplicatas")
        
        # Distribui√ß√£o target
        if 'salary' in df.columns:
            target_dist = df['salary'].value_counts(normalize=True)
            print(f"   üéØ Distribui√ß√£o target:")
            for value, percentage in target_dist.items():
                print(f"     ‚Ä¢ {value}: {percentage:.1f}%")
    
    def test_output_artifacts_validation(self):
        """Validar artefatos de sa√≠da gerados"""
        output_dir = Path("output")
        
        if output_dir.exists():
            # Estado do pipeline
            state_file = output_dir / "pipeline_state.json"
            if state_file.exists():
                with open(state_file, 'r') as f:
                    state = json.load(f)
                
                required_state_keys = ['data_source', 'models_count', 'performance_metrics', 'timestamp']
                for key in required_state_keys:
                    assert key in state, f"Chave ausente no estado: {key}"
                
                print(f"‚úÖ Estado do pipeline validado:")
                print(f"   üìä Fonte: {state['data_source']}")
                print(f"   ü§ñ Modelos: {state['models_count']}")
                print(f"   üìÖ Timestamp: {state['timestamp']}")
            
            # Resumo dos modelos
            models_file = output_dir / "models_summary.json"
            if models_file.exists():
                with open(models_file, 'r') as f:
                    models_summary = json.load(f)
                
                print(f"‚úÖ Resumo de modelos validado: {len(models_summary)} modelos")
        
        # Artefatos de ML
        processed_dir = Path("data/processed")
        if processed_dir.exists():
            model_files = list(processed_dir.glob("*_model.joblib"))
            print(f"‚úÖ Artefatos ML: {len(model_files)} modelos salvos")
    
    def test_comprehensive_data_science_report(self):
        """Gerar relat√≥rio completo de Data Science"""
        try:
            from main import HybridPipelineSQL
            
            print("\n" + "="*80)
            print("üìä RELAT√ìRIO COMPLETO DE DATA SCIENCE")
            print("="*80)
            
            pipeline = HybridPipelineSQL(force_csv=True, log_level="WARNING")
            results = pipeline.run()
            
            # Se√ß√£o 1: Dados
            print("\nüìã 1. DADOS:")
            df = results['df']
            print(f"   ‚Ä¢ Registros: {len(df):,}")
            print(f"   ‚Ä¢ Features: {len(df.columns)}")
            print(f"   ‚Ä¢ Fonte: {results['data_source'].upper()}")
            
            # Se√ß√£o 2: Modelos
            print("\nü§ñ 2. MACHINE LEARNING:")
            models = results['models']
            print(f"   ‚Ä¢ Modelos treinados: {len(models)}")
            
            # Se√ß√£o 3: Performance
            print("\nüìà 3. PERFORMANCE:")
            metrics = results['performance_metrics']
            print(f"   ‚Ä¢ Tempo total: {metrics['total_time']:.2f}s")
            print(f"   ‚Ä¢ Registros processados: {metrics['records_processed']:,}")
            
            # Se√ß√£o 4: Qualidade
            print("\n‚úÖ 4. QUALIDADE:")
            missing_data = df.isnull().sum().sum()
            duplicates = df.duplicated().sum()
            print(f"   ‚Ä¢ Dados ausentes: {missing_data}")
            print(f"   ‚Ä¢ Duplicatas: {duplicates}")
            
            print("\n" + "="*80)
            print("‚úÖ RELAT√ìRIO CONCLU√çDO")
            print("="*80)
            
        except ImportError:
            pytest.skip("Pipeline principal n√£o dispon√≠vel")