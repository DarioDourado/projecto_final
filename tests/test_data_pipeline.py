"""
üî¨ Testes do Pipeline de Dados
Valida√ß√£o completa de carregamento, limpeza e qualidade dos dados
"""

import pytest
import pandas as pd
import numpy as np
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent.parent / "src"))

class TestDataPipeline:
    """Testes do pipeline de dados"""
    
    def test_data_loading_sql_connection(self):
        """Testar conex√£o e carregamento SQL"""
        try:
            from src.data.loader import load_data, get_data_status
            
            # Testar status das fontes
            status = get_data_status()
            
            assert 'sql' in status
            assert 'csv' in status
            assert 'recommended' in status
            
            # Log dos resultados
            print(f"‚úÖ Status SQL: {status['sql']}")
            print(f"‚úÖ Status CSV: {status['csv']}")
            print(f"‚úÖ Recomendado: {status['recommended']}")
            
        except ImportError:
            pytest.skip("M√≥dulos de dados n√£o dispon√≠veis")
    
    def test_data_loading_hybrid_fallback(self):
        """Testar fallback autom√°tico SQL‚ÜíCSV"""
        try:
            from src.data.loader import load_data
            
            # Teste h√≠brido
            df_hybrid, source_hybrid = load_data(force_csv=False)
            
            assert df_hybrid is not None
            assert len(df_hybrid) > 0
            assert source_hybrid in ['sql', 'csv']
            
            # Teste CSV for√ßado
            df_csv, source_csv = load_data(force_csv=True)
            
            assert df_csv is not None
            assert source_csv == 'csv'
            assert len(df_csv) > 0
            
            print(f"‚úÖ H√≠brido: {len(df_hybrid):,} registros via {source_hybrid}")
            print(f"‚úÖ CSV: {len(df_csv):,} registros via {source_csv}")
            
        except ImportError:
            pytest.skip("M√≥dulos de dados n√£o dispon√≠veis")
    
    def test_data_quality_validation(self, sample_salary_data, expected_data_quality_metrics):
        """Validar qualidade dos dados carregados"""
        df = sample_salary_data
        
        # Valida√ß√µes b√°sicas
        assert len(df) > 0, "Dataset n√£o pode estar vazio"
        assert len(df.columns) >= 10, "Dataset deve ter pelo menos 10 colunas"
        
        # Validar tipos de dados
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=['object']).columns
        
        assert len(numeric_cols) >= 5, "Deve ter pelo menos 5 colunas num√©ricas"
        assert len(categorical_cols) >= 5, "Deve ter pelo menos 5 colunas categ√≥ricas"
        
        # Validar coluna target
        assert 'salary' in df.columns, "Coluna 'salary' deve existir"
        
        target_values = df['salary'].unique()
        expected_targets = ['<=50K', '>50K']
        assert all(val in expected_targets for val in target_values), f"Valores target inv√°lidos: {target_values}"
        
        # Distribui√ß√£o target
        target_dist = df['salary'].value_counts(normalize=True)
        assert target_dist['<=50K'] > 0.5, "Classe majorit√°ria deve ser <=50K"
        
        print(f"‚úÖ Qualidade validada: {len(df)} registros")
        print(f"‚úÖ Features num√©ricas: {len(numeric_cols)}")
        print(f"‚úÖ Features categ√≥ricas: {len(categorical_cols)}")
        print(f"‚úÖ Distribui√ß√£o target: {dict(target_dist)}")
    
    def test_data_preprocessing_steps(self, sample_salary_data):
        """Testar etapas de pr√©-processamento"""
        df = sample_salary_data.copy()
        
        # Simular dados ausentes
        df.loc[:10, 'workclass'] = np.nan
        df.loc[:5, 'occupation'] = np.nan
        
        # Validar detec√ß√£o de missing data
        missing_data = df.isnull().sum()
        cols_with_missing = missing_data[missing_data > 0].index.tolist()
        
        assert 'workclass' in cols_with_missing
        assert 'occupation' in cols_with_missing
        
        # Simular duplicatas
        df_with_dups = pd.concat([df, df.iloc[:10]], ignore_index=True)
        duplicates = df_with_dups.duplicated().sum()
        
        assert duplicates == 10, f"Esperado 10 duplicatas, encontrado {duplicates}"
        
        print(f"‚úÖ Missing data detectado: {cols_with_missing}")
        print(f"‚úÖ Duplicatas detectadas: {duplicates}")
    
    def test_feature_engineering_validation(self, sample_salary_data):
        """Validar engenharia de features"""
        df = sample_salary_data
        
        # Features essenciais para ML
        essential_features = ['age', 'education-num', 'hours-per-week', 'capital-gain']
        
        for feature in essential_features:
            assert feature in df.columns, f"Feature essencial ausente: {feature}"
            
            # Validar ranges sensatos
            if feature == 'age':
                assert df[feature].min() >= 15, "Idade m√≠nima inv√°lida"
                assert df[feature].max() <= 100, "Idade m√°xima inv√°lida"
            
            elif feature == 'education-num':
                assert df[feature].min() >= 1, "education-num deve ser >= 1"
                assert df[feature].max() <= 16, "education-num deve ser <= 16"
            
            elif feature == 'hours-per-week':
                assert df[feature].min() >= 1, "hours-per-week deve ser >= 1"
                assert df[feature].max() <= 100, "hours-per-week deve ser <= 100"
        
        print(f"‚úÖ Features essenciais validadas: {essential_features}")