# projeto_salario.py

# ================================================
# 1. IMPORTA√á√ÉO DE BIBLIOTECAS
# ================================================
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os

# Configura√ß√£o global para fundo transparente
plt.rcParams['figure.facecolor'] = 'none'
plt.rcParams['axes.facecolor'] = 'none'
plt.rcParams['savefig.facecolor'] = 'none'

# ================================================
# 2. CARREGAMENTO E LIMPEZA DOS DADOS
# ================================================
df = pd.read_csv('4-Carateristicas_salario.csv')
df = df.drop_duplicates()

print("\nResumo dos dados:")
print(df.info())
print("\nDescri√ß√£o estat√≠stica:")
print(df.describe())
print("\nValores ausentes por coluna:")
print(df.isnull().sum())

# ================================================
# 2.1. LIMPEZA E TIPAGEM DOS DADOS
# ================================================
print("\n" + "="*60)
print("LIMPEZA E TIPAGEM DOS DADOS")
print("="*60)

def limpar_e_tipar_dados(df):
    """Limpar dados e aplicar tipagem correta √†s colunas"""
    
    # Fazer c√≥pia para n√£o alterar o original
    df_clean = df.copy()
    
    # Limpar espa√ßos em branco em todas as colunas de texto
    for col in df_clean.select_dtypes(include=['object']).columns:
        df_clean[col] = df_clean[col].astype(str).str.strip()
    
    # Tratar valores '?' como NaN
    df_clean = df_clean.replace('?', pd.NA)
    
    print("üîß Aplicando tipagem correta √†s colunas...")
    
    # ===== TIPAGEM DAS VARI√ÅVEIS NUM√âRICAS =====
    numerical_columns_types = {
        'age': 'int16',           # Idade: 17-90 (int16 suficiente)
        'fnlwgt': 'int32',        # Peso final: pode ser grande (int32)
        'education-num': 'int8',  # Anos educa√ß√£o: 1-16 (int8 suficiente)
        'capital-gain': 'int32',  # Ganho capital: pode ser grande
        'capital-loss': 'int16',  # Perda capital: menor range
        'hours-per-week': 'int8'  # Horas semana: 1-99 (int8 suficiente)
    }
    
    for col, dtype in numerical_columns_types.items():
        if col in df_clean.columns:
            try:
                # Converter para num√©rico primeiro, depois para o tipo espec√≠fico
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                df_clean[col] = df_clean[col].astype(dtype)
                print(f"‚úÖ {col}: convertido para {dtype}")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao converter {col}: {e}")
    
    # ===== TIPAGEM DAS VARI√ÅVEIS CATEG√ìRICAS =====
    categorical_columns = [
        'workclass', 'education', 'marital-status', 'occupation',
        'relationship', 'race', 'sex', 'native-country', 'salary'
    ]
    
    for col in categorical_columns:
        if col in df_clean.columns:
            try:
                # Converter para categoria para economizar mem√≥ria
                df_clean[col] = df_clean[col].astype('category')
                print(f"‚úÖ {col}: convertido para category")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao converter {col}: {e}")
    
    # ===== VALIDA√á√ÉO DE RANGES =====
    print("\nüîç Validando ranges das vari√°veis...")
    
    # Validar idade
    if 'age' in df_clean.columns:
        invalid_age = (df_clean['age'] < 17) | (df_clean['age'] > 100)
        if invalid_age.any():
            print(f"‚ö†Ô∏è Encontradas {invalid_age.sum()} idades inv√°lidas (fora de 17-100)")
            df_clean.loc[invalid_age, 'age'] = pd.NA
    
    # Validar anos de educa√ß√£o
    if 'education-num' in df_clean.columns:
        invalid_edu = (df_clean['education-num'] < 1) | (df_clean['education-num'] > 16)
        if invalid_edu.any():
            print(f"‚ö†Ô∏è Encontrados {invalid_edu.sum()} anos de educa√ß√£o inv√°lidos (fora de 1-16)")
            df_clean.loc[invalid_edu, 'education-num'] = pd.NA
    
    # Validar horas por semana
    if 'hours-per-week' in df_clean.columns:
        invalid_hours = (df_clean['hours-per-week'] < 1) | (df_clean['hours-per-week'] > 99)
        if invalid_hours.any():
            print(f"‚ö†Ô∏è Encontradas {invalid_hours.sum()} horas/semana inv√°lidas (fora de 1-99)")
            df_clean.loc[invalid_hours, 'hours-per-week'] = pd.NA
    
    # Validar ganhos/perdas de capital (n√£o podem ser negativos)
    for col in ['capital-gain', 'capital-loss']:
        if col in df_clean.columns:
            invalid_capital = df_clean[col] < 0
            if invalid_capital.any():
                print(f"‚ö†Ô∏è Encontrados {invalid_capital.sum()} valores negativos em {col}")
                df_clean.loc[invalid_capital, col] = 0
    
    return df_clean

def get_memory_usage(df):
    """Calcular uso de mem√≥ria do DataFrame"""
    memory_usage = df.memory_usage(deep=True).sum()
    return memory_usage / 1024 / 1024  # MB

# Mostrar uso de mem√≥ria antes da otimiza√ß√£o
memory_before = get_memory_usage(df)
print(f"üíæ Uso de mem√≥ria antes da otimiza√ß√£o: {memory_before:.2f} MB")

# Aplicar limpeza e tipagem
df = limpar_e_tipar_dados(df)

# Mostrar uso de mem√≥ria ap√≥s otimiza√ß√£o
memory_after = get_memory_usage(df)
print(f"üíæ Uso de mem√≥ria ap√≥s otimiza√ß√£o: {memory_after:.2f} MB")
print(f"üìâ Redu√ß√£o: {((memory_before - memory_after) / memory_before * 100):.1f}%")

print("\n‚úÖ Dados limpos e tipados com sucesso!")
print("\nInfo dos tipos de dados:")
print(df.dtypes)

# Verificar valores ausentes ap√≥s limpeza
missing_values = df.isnull().sum()
if missing_values.any():
    print(f"\n‚ö†Ô∏è Valores ausentes ap√≥s limpeza:")
    print(missing_values[missing_values > 0])

# ================================================
# 3. AN√ÅLISE EXPLORAT√ìRIA DE DADOS (EDA)
# ================================================
# Criar diret√≥rio para imagens se n√£o existir
os.makedirs("imagens", exist_ok=True)

# Distribui√ß√µes das vari√°veis num√©ricas
numerical_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']
for col in numerical_cols:
    plt.figure(figsize=(6, 4))
    # Configurar fundo transparente
    plt.gca().patch.set_alpha(0.0)
    plt.gcf().patch.set_alpha(0.0)
    
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f'Distribui√ß√£o de {col}')
    plt.tight_layout()
    plt.savefig(f'imagens/hist_{col}.png', transparent=True, bbox_inches='tight')
    plt.close()

# Matriz de correla√ß√£o
corr = df[numerical_cols].corr()
plt.figure(figsize=(8, 6))
# Configurar fundo transparente
plt.gca().patch.set_alpha(0.0)
plt.gcf().patch.set_alpha(0.0)

sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title("Matriz de Correla√ß√£o entre Vari√°veis Num√©ricas")
plt.tight_layout()
plt.savefig('imagens/correlacao.png', transparent=True, bbox_inches='tight')
plt.close()

# Distribui√ß√£o de sal√°rio por vari√°veis categ√≥ricas
categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 
                    'relationship', 'race', 'sex', 'native-country']
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    # Configurar fundo transparente
    plt.gca().patch.set_alpha(0.0)
    plt.gcf().patch.set_alpha(0.0)
    
    sns.countplot(data=df, x=col, hue='salary')
    plt.title(f'Distribui√ß√£o de Sal√°rio por {col}')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f'imagens/bar_{col}.png', transparent=True, bbox_inches='tight')
    plt.close()

print("\nGr√°ficos salvos na pasta 'imagens'")

# ================================================
# 4. PR√â-PROCESSAMENTO
# ================================================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# Separar vari√°veis num√©ricas e categ√≥ricas
numerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']
categorical_features = ['workclass', 'education', 'marital-status', 'occupation',
                        'relationship', 'race', 'sex', 'native-country']

# DIAGN√ìSTICO DETALHADO DOS DADOS
print("\n" + "="*60)
print("DIAGN√ìSTICO DETALHADO DOS DADOS")
print("="*60)

# Verificar distribui√ß√£o original antes da codifica√ß√£o
print("\nDistribui√ß√£o original de salary:")
print(df['salary'].value_counts())
print("Valores √∫nicos originais:", df['salary'].unique())

# Verificar se h√° espa√ßos ou caracteres especiais
print("\nAn√°lise detalhada dos valores de salary:")
for valor in df['salary'].unique():
    print(f"'{valor}' (tipo: {type(valor)}, comprimento: {len(str(valor))})")

# Limpar valores de salary primeiro
df['salary'] = df['salary'].astype(str).str.strip()
print("\nAp√≥s limpeza de espa√ßos:")
print(df['salary'].value_counts())

# Codificar vari√°vel-alvo com verifica√ß√£o mais robusta
def codificar_salary(valor):
    valor = str(valor).strip().lower()
    if '>50k' in valor or valor == '>50k':
        return 1
    elif '<=50k' in valor or valor == '<=50k':
        return 0
    else:
        print(f"‚ö†Ô∏è Valor n√£o reconhecido: '{valor}'")
        return 0

df['salary'] = df['salary'].apply(codificar_salary)

# Verificar distribui√ß√£o ap√≥s codifica√ß√£o
print("\nDistribui√ß√£o ap√≥s codifica√ß√£o:")
print(df['salary'].value_counts())
print("Percentual de cada classe:")
print(df['salary'].value_counts(normalize=True) * 100)

# Verificar se h√° ambas as classes
if df['salary'].nunique() < 2:
    print("‚ö†Ô∏è ERRO: Apenas uma classe encontrada nos dados!")
    print("Valores √∫nicos em salary:", df['salary'].unique())
    
    # Investigar mais a fundo
    df_original = pd.read_csv('4-Carateristicas_salario.csv')
    print("\nValores √∫nicos no arquivo original:")
    for valor in df_original['salary'].unique():
        print(f"'{valor}' (aparece {(df_original['salary'] == valor).sum()} vezes)")
    
    # Tentar uma codifica√ß√£o diferente
    print("\nTentando codifica√ß√£o alternativa...")
    df['salary'] = df_original['salary'].apply(lambda x: 1 if '>' in str(x) else 0)
    print("Nova distribui√ß√£o:")
    print(df['salary'].value_counts())

else:
    print("‚úÖ Ambas as classes presentes nos dados")

# Separar X e y
X = df[numerical_features + categorical_features]
y = df['salary']

# Verificar valores ausentes
print(f"\nValores ausentes em X: {X.isnull().sum().sum()}")
print(f"Valores ausentes em y: {y.isnull().sum()}")

# Tratar valores '?' como NaN primeiro
X = X.replace('?', pd.NA)
print(f"Valores ausentes ap√≥s tratar '?': {X.isnull().sum().sum()}")

# Remover registros com valores ausentes se necess√°rio
if X.isnull().sum().sum() > 0:
    print("Removendo registros com valores ausentes...")
    mask = ~(X.isnull().any(axis=1) | y.isnull())
    X = X[mask]
    y = y[mask]
    print(f"Registros restantes: {len(X)}")

# Verificar distribui√ß√£o final antes da divis√£o
print(f"\nDistribui√ß√£o final de y antes da divis√£o:")
print(y.value_counts())
print("Classes √∫nicas:", y.unique())

# PARAR SE N√ÉO H√Å AMBAS AS CLASSES
if y.nunique() < 2:
    print("\n‚ùå ERRO CR√çTICO: Imposs√≠vel continuar com apenas uma classe!")
    print("Verifique os dados originais e a codifica√ß√£o da vari√°vel 'salary'")
    exit()

# Pr√©-processador para colunas (OneHot para categ√≥ricas, StandardScaler para num√©ricas)
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)
    ])

# Divis√£o treino/teste com stratify para manter propor√ß√£o das classes
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42, 
    stratify=y  # Importante: mant√©m propor√ß√£o das classes
)

# Verificar distribui√ß√£o nas parti√ß√µes
print(f"\nDistribui√ß√£o em y_train:")
print(y_train.value_counts())
print(f"\nDistribui√ß√£o em y_test:")
print(y_test.value_counts())

# Ajustar transforma√ß√µes
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Novo: Garantir que X_test_processed √© do tipo float
X_test_processed = X_test_processed.astype(float)

print("\nPr√©-processamento completo. Formato final dos dados:")
print("X_train:", X_train_processed.shape)
print("X_test:", X_test_processed.shape)
print("y_train classes √∫nicas:", y_train.unique())
print("y_test classes √∫nicas:", y_test.unique())

# ================================================
# 5. BALANCEAMENTO DOS DADOS (SMOTE) - CONDICIONAL
# ================================================
print("\n" + "="*60)
print("BALANCEAMENTO DOS DADOS")
print("="*60)

# Verificar se precisa de balanceamento
class_counts = y_train.value_counts()
minority_ratio = min(class_counts) / max(class_counts)

print(f"Propor√ß√£o da classe minorit√°ria: {minority_ratio:.2f}")

if minority_ratio < 0.3:  # Se a classe minorit√°ria representa menos de 30%
    print("Dataset desbalanceado. Aplicando SMOTE...")
    from imblearn.over_sampling import SMOTE
    
    # Verificar se h√° pelo menos 2 classes antes do SMOTE
    if len(y_train.unique()) >= 2:
        sm = SMOTE(random_state=42)
        X_train_processed, y_train = sm.fit_resample(X_train_processed, y_train)
        
        print("‚úÖ SMOTE aplicado com sucesso")
        print("Distribui√ß√£o ap√≥s SMOTE:")
        print(y_train.value_counts())
    else:
        print("‚ö†Ô∏è Imposs√≠vel aplicar SMOTE: apenas uma classe nos dados de treino")
else:
    print("Dataset j√° balanceado. SMOTE n√£o necess√°rio.")

# ================================================
# 6. MODELAGEM PREDITIVA
# ================================================
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import joblib

# Fun√ß√£o para treinar e avaliar modelos
def avaliar_modelo(nome, modelo, X_train, y_train, X_test, y_test):
    # Verificar se h√° pelo menos 2 classes nos dados de treino
    if len(y_train.unique()) < 2:
        print(f"\n‚ö†Ô∏è {nome}: Imposs√≠vel treinar - apenas uma classe nos dados de treino")
        print(f"Classes em y_train: {y_train.unique()}")
        return None
    
    try:
        modelo.fit(X_train, y_train)
        y_pred = modelo.predict(X_test)
        print(f"\n{'='*20}\n{nome}\n{'='*20}")
        print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
        print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))
        print("Matriz de Confus√£o:")
        print(confusion_matrix(y_test, y_pred))
        return modelo
    except Exception as e:
        print(f"\n‚ö†Ô∏è Erro ao treinar {nome}: {str(e)}")
        return None

# Inicializar modelos
modelos = {
    "Regress√£o Log√≠stica": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),
    "k-NN (k=5)": KNeighborsClassifier(n_neighbors=5)
}

# Avaliar todos os modelos e salvar
modelos_treinados = {}
for nome, modelo in modelos.items():
    modelo_treinado = avaliar_modelo(nome, modelo, X_train_processed, y_train, X_test_processed, y_test)
    if modelo_treinado is not None:
        modelos_treinados[nome] = modelo_treinado

# ================================================
# SALVAR MODELOS E PREPROCESSOR
# ================================================
print("\n" + "="*60)
print("SALVANDO MODELOS E PREPROCESSOR")
print("="*60)

# Criar diret√≥rio models se n√£o existir
os.makedirs("models", exist_ok=True)

# Salvar o modelo Random Forest (melhor para interpretabilidade)
if "Random Forest" in modelos_treinados:
    joblib.dump(modelos_treinados["Random Forest"], "random_forest_model.joblib")
    print("‚úÖ Modelo Random Forest salvo como 'random_forest_model.joblib'")

# Salvar o preprocessor
joblib.dump(preprocessor, "preprocessor.joblib")
print("‚úÖ Preprocessor salvo como 'preprocessor.joblib'")

# Gerar nomes das features ap√≥s o preprocessamento
ohe = preprocessor.named_transformers_['cat']
cat_feature_names = ohe.get_feature_names_out(categorical_features)
feature_names = numerical_features + list(cat_feature_names)

# Salvar informa√ß√µes sobre as features
feature_info = {
    'numerical_features': numerical_features,
    'categorical_features': categorical_features,
    'feature_names': feature_names
}
joblib.dump(feature_info, "feature_info.joblib")
print("‚úÖ Informa√ß√µes das features salvas como 'feature_info.joblib'")

# Salvar alguns dados de exemplo para testes
sample_data = X_test.head(5)
joblib.dump(sample_data, "sample_data.joblib")
print("‚úÖ Dados de exemplo salvos como 'sample_data.joblib'")

print(f"\nüìÅ Todos os arquivos salvos com sucesso!")
print("Arquivos dispon√≠veis para o dashboard:")
print("- random_forest_model.joblib")
print("- preprocessor.joblib") 
print("- feature_info.joblib")
print("- sample_data.joblib")

# ================================================
# 7. CLUSTERING E REGRAS DE ASSOCIA√á√ÉO
# ================================================
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd

# Usar PCA para reduzir a dimensionalidade para 2D para visualiza√ß√£o
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed)

# KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_pca)

# Plot dos clusters
plt.figure(figsize=(8, 6))
# Configurar fundo transparente
plt.gca().patch.set_alpha(0.0)
plt.gcf().patch.set_alpha(0.0)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', s=10)
plt.title("Visualiza√ß√£o dos Clusters com KMeans (PCA 2D)")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.tight_layout()
plt.savefig("imagens/kmeans_clusters.png", transparent=True, bbox_inches='tight')
plt.close()
print("\nClustering com KMeans conclu√≠do. Gr√°fico salvo em 'imagens/kmeans_clusters.png'.")

# -------------------------------------
# Regras de associa√ß√£o (via Apriori)
# -------------------------------------
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Criar um dataset amostral com transa√ß√µes bin√°rias (apenas para demonstra√ß√£o simples)
amostras = df[['education', 'occupation', 'relationship', 'sex', 'salary']].astype(str).values.tolist()
te = TransactionEncoder()
transacoes = te.fit(amostras).transform(amostras)
df_transacoes = pd.DataFrame(transacoes, columns=te.columns_)

# Aplicar Apriori
freq_items = apriori(df_transacoes, min_support=0.05, use_colnames=True)
regras = association_rules(freq_items, metric="confidence", min_threshold=0.6)

# Mostrar as 5 principais regras
print("\nTop 5 Regras de Associa√ß√£o:")
print(regras[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())

# ================================================
# 8. INTERPRETA√á√ÉO DOS MODELOS
# ================================================
print("\n" + "="*60)
print("INTERPRETA√á√ÉO DOS MODELOS")
print("="*60)

if "Random Forest" in modelos_treinados:
    # -------------------------------
    # FEATURE IMPORTANCE - Random Forest
    # -------------------------------
    importances = modelos_treinados["Random Forest"].feature_importances_
    
    # Obter nomes das colunas ap√≥s transforma√ß√£o
    ohe = preprocessor.named_transformers_['cat']
    cat_feature_names = ohe.get_feature_names_out(categorical_features)
    feature_names = numerical_features + list(cat_feature_names)
    
    # Combinar com os valores
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    
    # Mostrar top 15
    print("\nTop 15 Features mais importantes (Random Forest):")
    print(feature_importance_df.head(15))
    
    # Plot com fundo transparente
    plt.figure(figsize=(10, 6))
    # Configurar fundo transparente
    plt.gca().patch.set_alpha(0.0)
    plt.gcf().patch.set_alpha(0.0)
    
    sns.barplot(data=feature_importance_df.head(15), x='Importance', y='Feature')
    plt.title("Import√¢ncia das Features - Random Forest")
    plt.tight_layout()
    plt.savefig("imagens/feature_importance_rf.png", transparent=True, bbox_inches='tight')
    plt.close()
    
    # -------------------------------
    # COEFICIENTES - Regress√£o Log√≠stica (se dispon√≠vel)
    # -------------------------------
    if "Regress√£o Log√≠stica" in modelos_treinados:
        coef = modelos_treinados["Regress√£o Log√≠stica"].coef_[0]
        
        coef_df = pd.DataFrame({
            'Feature': feature_names,
            'Coefficient': coef
        }).sort_values(by='Coefficient', key=abs, ascending=False)
        
        print("\nTop 15 Coeficientes mais importantes (Regress√£o Log√≠stica):")
        print(coef_df.head(15))
        
        # Plot com fundo transparente
        plt.figure(figsize=(10, 6))
        # Configurar fundo transparente
        plt.gca().patch.set_alpha(0.0)
        plt.gcf().patch.set_alpha(0.0)
        
        sns.barplot(data=coef_df.head(15), x='Coefficient', y='Feature')
        plt.title("Coeficientes - Regress√£o Log√≠stica")
        plt.tight_layout()
        plt.savefig("imagens/coefficients_lr.png", transparent=True, bbox_inches='tight')
        plt.close()
    
    # -------------------------------
    # SHAP VALUES (corrigido)
    # -------------------------------
    try:
        import shap
        
        # Converter para array denso se necess√°rio e garantir tipo float
        X_train_shap = X_train_processed
        if hasattr(X_train_shap, 'toarray'):
            X_train_shap = X_train_shap.toarray()
        
        # Garantir que √© float64
        X_train_shap = X_train_shap.astype(np.float64)
        
        # Usar apenas uma amostra menor para SHAP (mais r√°pido)
        sample_size = min(1000, X_train_shap.shape[0])
        X_sample = X_train_shap[:sample_size]
        
        print(f"\nGerando SHAP values para {sample_size} amostras...")
        
        explainer = shap.TreeExplainer(modelos_treinados["Random Forest"])
        shap_values = explainer.shap_values(X_sample)
        
        # Se o modelo retorna valores SHAP para cada classe, usar a classe positiva
        if isinstance(shap_values, list):
            shap_values_plot = shap_values[1]  # Classe positiva (>50K)
        else:
            shap_values_plot = shap_values
        
        # Summary plot com fundo transparente
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values_plot, X_sample, feature_names=feature_names, show=False)
        plt.tight_layout()
        plt.savefig("imagens/shap_summary_rf.png", transparent=True, bbox_inches='tight')
        plt.close()
        
        # Bar plot SHAP com fundo transparente
        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_values_plot, X_sample, feature_names=feature_names, plot_type="bar", show=False)
        plt.tight_layout()
        plt.savefig("imagens/shap_bar_rf.png", transparent=True, bbox_inches='tight')
        plt.close()
        
        print("‚úÖ Gr√°ficos SHAP gerados com sucesso.")
        
    except ImportError:
        print("\n‚ö†Ô∏è Pacote SHAP n√£o instalado. Execute: pip install shap")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Erro ao gerar gr√°ficos SHAP: {str(e)}")
        print("Continuando sem SHAP...")

else:
    print("‚ö†Ô∏è Modelo Random Forest n√£o foi treinado com sucesso.")

print("\nüéâ Pipeline completo executado!")
print("\nüìÅ Arquivos gerados:")
print("- Dados: 4-Carateristicas_salario.csv")
print("- Modelo: random_forest_model.joblib")
print("- Preprocessor: preprocessor.joblib")
print("- Features: feature_info.joblib")
print("- Amostras: sample_data.joblib")
print("- Gr√°ficos: pasta imagens/ (todos com fundo transparente)")
