# projeto_salario.py

# ================================================
# 1. IMPORTA√á√ÉO DE BIBLIOTECAS
# ================================================
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os
import logging

# ================================================
# 1.1. CONFIGURA√á√ÉO DE LOGGING
# ================================================
class EmojiFormatter(logging.Formatter):
    """Formatter personalizado com emojis"""
    
    emoji_mapping = {
        'DEBUG': 'üîç',
        'INFO': '‚ÑπÔ∏è',
        'WARNING': '‚ö†Ô∏è',
        'ERROR': '‚ùå',
        'CRITICAL': 'üö®',
        'SUCCESS': '‚úÖ'
    }
    
    def format(self, record):
        emoji = self.emoji_mapping.get(record.levelname, '‚ÑπÔ∏è')
        return f"{emoji} {record.getMessage()}"

# Aplicar o formatter personalizado
logger = logging.getLogger()
handler = logging.StreamHandler()
handler.setFormatter(EmojiFormatter())
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# Remover handlers padr√£o para evitar duplica√ß√£o
logger.handlers = [handler]

# ================================================
# 1.2. FUN√á√ïES DE LOGGING AUXILIARES (DEFINIR PRIMEIRO)
# ================================================
def log_success(message):
    """Log com checkmark verde"""
    logging.info(f"‚úÖ {message}")

def log_function_start(function_name):
    """Log in√≠cio de fun√ß√£o"""
    logging.info(f"üîÑ Iniciando: {function_name}")

def log_function_end(function_name):
    """Log fim de fun√ß√£o com sucesso"""
    logging.info(f"‚úÖ Conclu√≠do: {function_name}")

def log_function(func):
    """Decorator para logging de in√≠cio e fim de fun√ß√£o"""
    def wrapper(*args, **kwargs):
        logging.info(f"üîÑ Iniciando: {func.__name__}")
        result = func(*args, **kwargs)
        logging.info(f"‚úÖ Conclu√≠do: {func.__name__}")
        return result
    return wrapper

def get_memory_usage(df):
    """Calcular uso de mem√≥ria do DataFrame"""
    memory_usage = df.memory_usage(deep=True).sum()
    return memory_usage / 1024 / 1024  # MB

# Configura√ß√£o global para fundo transparente
plt.rcParams['figure.facecolor'] = 'none'
plt.rcParams['axes.facecolor'] = 'none'
plt.rcParams['savefig.facecolor'] = 'none'

# ================================================
# 2. CARREGAMENTO E LIMPEZA DOS DADOS
# ================================================
df = pd.read_csv('4-Carateristicas_salario.csv')
df = df.drop_duplicates()
logging.info("‚úÖ Dados carregados e duplicatas removidas")

logging.info("\nResumo dos dados:")
logging.info(df.info())
logging.info("\nDescri√ß√£o estat√≠stica:")
logging.info(df.describe())
logging.info("\nValores ausentes por coluna:")
logging.info(df.isnull().sum())

# ================================================
# 2.1. LIMPEZA E TIPAGEM DOS DADOS
# ================================================
logging.info("\n" + "="*60)
logging.info("LIMPEZA E TIPAGEM DOS DADOS")
logging.info("="*60)

def limpar_e_tipar_dados(df):
    """Limpar dados e aplicar tipagem correta √†s colunas"""
    log_function_start("Limpeza e tipagem de dados")
    
    # Fazer c√≥pia para n√£o alterar o original
    df_clean = df.copy()
    
    # Limpar espa√ßos em branco em todas as colunas de texto
    for col in df_clean.select_dtypes(include=['object']).columns:
        df_clean[col] = df_clean[col].astype(str).str.strip()
    
    # Tratar valores '?' como NaN
    df_clean = df_clean.replace('?', pd.NA)
    
    logging.info("üîß Aplicando tipagem correta √†s colunas...")
    
    # ===== TIPAGEM DAS VARI√ÅVEIS NUM√âRICAS =====
    numerical_columns_types = {
        'age': 'int16',           # Idade: 17-90 (int16 suficiente)
        'fnlwgt': 'int32',        # Peso final: pode ser grande (int32)
        'education-num': 'int8',  # Anos educa√ß√£o: 1-16 (int8 suficiente)
        'capital-gain': 'int32',  # Ganho capital: pode ser grande
        'capital-loss': 'int16',  # Perda capital: menor range
        'hours-per-week': 'int8'  # Horas semana: 1-99 (int8 suficiente)
    }
    
    for col, dtype in numerical_columns_types.items():
        if col in df_clean.columns:
            try:
                # Converter para num√©rico primeiro, depois para o tipo espec√≠fico
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                df_clean[col] = df_clean[col].astype(dtype)
                logging.info(f"‚úÖ {col}: convertido para {dtype}")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Erro ao converter {col}: {e}")
    
    # ===== TIPAGEM DAS VARI√ÅVEIS CATEG√ìRICAS =====
    categorical_columns = [
        'workclass', 'education', 'marital-status', 'occupation',
        'relationship', 'race', 'sex', 'native-country', 'salary'
    ]
    
    for col in categorical_columns:
        if col in df_clean.columns:
            try:
                # Converter para categoria para economizar mem√≥ria
                df_clean[col] = df_clean[col].astype('category')
                logging.info(f"‚úÖ {col}: convertido para category")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Erro ao converter {col}: {e}")
    
    # ===== VALIDA√á√ÉO DE RANGES =====
    logging.info("\nüîç Validando ranges das vari√°veis...")
    
    # Validar idade
    if 'age' in df_clean.columns:
        invalid_age = (df_clean['age'] < 17) | (df_clean['age'] > 100)
        if invalid_age.any():
            logging.warning(f"‚ö†Ô∏è Encontradas {invalid_age.sum()} idades inv√°lidas (fora de 17-100)")
            df_clean.loc[invalid_age, 'age'] = pd.NA

    # Validar anos de educa√ß√£o
    if 'education-num' in df_clean.columns:
        invalid_edu = (df_clean['education-num'] < 1) | (df_clean['education-num'] > 16)
        if invalid_edu.any():
            logging.warning(f"‚ö†Ô∏è Encontrados {invalid_edu.sum()} anos de educa√ß√£o inv√°lidos (fora de 1-16)")
            df_clean.loc[invalid_edu, 'education-num'] = pd.NA

    # Validar horas por semana
    if 'hours-per-week' in df_clean.columns:
        invalid_hours = (df_clean['hours-per-week'] < 1) | (df_clean['hours-per-week'] > 99)
        if invalid_hours.any():
            logging.warning(f"‚ö†Ô∏è Encontradas {invalid_hours.sum()} horas/semana inv√°lidas (fora de 1-99)")
            df_clean.loc[invalid_hours, 'hours-per-week'] = pd.NA

    # Validar ganhos/perdas de capital (n√£o podem ser negativos)
    for col in ['capital-gain', 'capital-loss']:
        if col in df_clean.columns:
            invalid_capital = df_clean[col] < 0
            if invalid_capital.any():
                logging.warning(f"‚ö†Ô∏è Encontrados {invalid_capital.sum()} valores negativos em {col}")
                df_clean.loc[invalid_capital, col] = 0
    
    log_function_end("Limpeza e tipagem de dados")
    return df_clean, numerical_columns_types, categorical_columns

# Mostrar uso de mem√≥ria antes da otimiza√ß√£o
memory_before = get_memory_usage(df)
logging.info(f"üíæ Uso de mem√≥ria antes da otimiza√ß√£o: {memory_before:.2f} MB")

# Aplicar limpeza e tipagem
df, numerical_columns_types, categorical_columns = limpar_e_tipar_dados(df)
logging.info("‚úÖ Limpeza e tipagem de dados conclu√≠da")

# Mostrar uso de mem√≥ria ap√≥s otimiza√ß√£o
memory_after = get_memory_usage(df)
logging.info(f"üíæ Uso de mem√≥ria ap√≥s otimiza√ß√£o: {memory_after:.2f} MB")
logging.info(f"üìâ Redu√ß√£o: {((memory_before - memory_after) / memory_before * 100):.1f}%")

logging.info("\n‚úÖ Dados limpos e tipados com sucesso!")
logging.info("\nInfo dos tipos de dados:")
logging.info(df.dtypes)

# Verificar valores ausentes ap√≥s limpeza
missing_values = df.isnull().sum()
if missing_values.any():
    logging.warning(f"\n‚ö†Ô∏è Valores ausentes ap√≥s limpeza:")
    logging.warning(missing_values[missing_values > 0])

# Agora podemos usar numerical_columns_types.keys() sem erro
numerical_columns = list(numerical_columns_types.keys())

# ================================================
# 2.2. REMO√á√ÉO DE OUTLIERS
# ================================================
from scipy.stats import zscore

@log_function
def remover_outliers(df, columns, threshold=3):
    """Remove outliers com base no Z-score"""
    df_filtered = df.copy()
    outliers_removed = 0
    
    for col in columns:
        if col in df_filtered.columns and pd.api.types.is_numeric_dtype(df_filtered[col]):
            # Calcular Z-scores apenas para valores n√£o nulos
            valid_mask = df_filtered[col].notna()
            if valid_mask.sum() > 0:  # Se h√° valores v√°lidos
                z_scores = np.abs(zscore(df_filtered.loc[valid_mask, col]))
                outlier_mask = z_scores > threshold
                
                # Contar outliers antes de remover
                outliers_count = outlier_mask.sum()
                outliers_removed += outliers_count
                
                # Remover outliers
                outlier_indices = df_filtered.loc[valid_mask].index[outlier_mask]
                df_filtered = df_filtered.drop(outlier_indices)
                
                logging.info(f"üîç {col}: {outliers_count} outliers removidos (Z-score > {threshold})")
    
    logging.info(f"üìä Total de registros removidos: {outliers_removed}")
    logging.info(f"üìä Registros restantes: {len(df_filtered)} de {len(df)} originais")
    
    return df_filtered

# Aplicar remo√ß√£o de outliers
logging.info("\n" + "="*60)
logging.info("REMO√á√ÉO DE OUTLIERS")
logging.info("="*60)

# Verificar quais colunas num√©ricas existem no DataFrame
existing_numerical_cols = [col for col in numerical_columns if col in df.columns]
logging.info(f"Colunas num√©ricas encontradas: {existing_numerical_cols}")

if existing_numerical_cols:
    df_before_outliers = len(df)
    df = remover_outliers(df, existing_numerical_cols)
    df_after_outliers = len(df)
    
    reduction_percent = ((df_before_outliers - df_after_outliers) / df_before_outliers) * 100
    logging.info(f"üìâ Redu√ß√£o do dataset: {reduction_percent:.1f}%")
    
    # Verificar se ainda temos dados suficientes
    if len(df) < 1000:
        logging.warning("‚ö†Ô∏è ATEN√á√ÉO: Dataset muito pequeno ap√≥s remo√ß√£o de outliers!")
        logging.warning("Considere usar um threshold maior ou m√©todos alternativos.")
    else:
        logging.info("‚úÖ Outliers removidos com sucesso")
else:
    logging.warning("‚ö†Ô∏è Nenhuma coluna num√©rica encontrada para remo√ß√£o de outliers")

# ================================================
# 3. AN√ÅLISE EXPLORAT√ìRIA DE DADOS (EDA)
# ================================================
# Criar diret√≥rio para imagens se n√£o existir
os.makedirs("imagens", exist_ok=True)

# Distribui√ß√µes das vari√°veis num√©ricas
numerical_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']
for col in numerical_cols:
    plt.figure(figsize=(6, 4))
    # Configurar fundo transparente
    plt.gca().patch.set_alpha(0.0)
    plt.gcf().patch.set_alpha(0.0)
    
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f'Distribui√ß√£o de {col}')
    plt.tight_layout()
    plt.savefig(f'imagens/hist_{col}.png', transparent=True, bbox_inches='tight')
    plt.close()

# Matriz de correla√ß√£o
corr = df[numerical_cols].corr()
plt.figure(figsize=(8, 6))
# Configurar fundo transparente
plt.gca().patch.set_alpha(0.0)
plt.gcf().patch.set_alpha(0.0)

sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title("Matriz de Correla√ß√£o entre Vari√°veis Num√©ricas")
plt.tight_layout()
plt.savefig('imagens/correlacao.png', transparent=True, bbox_inches='tight')
plt.close()

# Distribui√ß√£o de sal√°rio por vari√°veis categ√≥ricas
categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 
                    'relationship', 'race', 'sex', 'native-country']
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    # Configurar fundo transparente
    plt.gca().patch.set_alpha(0.0)
    plt.gcf().patch.set_alpha(0.0)
    
    sns.countplot(data=df, x=col, hue='salary')
    plt.title(f'Distribui√ß√£o de Sal√°rio por {col}')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f'imagens/bar_{col}.png', transparent=True, bbox_inches='tight')
    plt.close()

logging.info("‚úÖ Gr√°ficos EDA gerados e salvos")

# ================================================
# 4. PR√â-PROCESSAMENTO
# ================================================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# Separar vari√°veis num√©ricas e categ√≥ricas
numerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']
categorical_features = ['workclass', 'education', 'marital-status', 'occupation',
                        'relationship', 'race', 'sex', 'native-country']

# DIAGN√ìSTICO DETALHADO DOS DADOS
logging.info("\n" + "="*60)
logging.info("DIAGN√ìSTICO DETALHADO DOS DADOS")
logging.info("="*60)

logging.info("\nDistribui√ß√£o original de salary:")
logging.info(df['salary'].value_counts())
logging.info(f"Valores √∫nicos originais: {df['salary'].unique()}")

# Verificar se h√° espa√ßos ou caracteres especiais
logging.info("\nAn√°lise detalhada dos valores de salary:")
for valor in df['salary'].unique():
    logging.info(f"'{valor}' (tipo: {type(valor)}, comprimento: {len(str(valor))})")

# Limpar valores de salary primeiro
df['salary'] = df['salary'].astype(str).str.strip()
logging.info("\nAp√≥s limpeza de espa√ßos:")
logging.info(df['salary'].value_counts())

# Codificar vari√°vel-alvo com verifica√ß√£o mais robusta
def codificar_salary(valor):
    valor = str(valor).strip().lower()
    if '>50k' in valor or valor == '>50k':
        return 1
    elif '<=50k' in valor or valor == '<=50k':
        return 0
    else:
        logging.warning(f"‚ö†Ô∏è Valor n√£o reconhecido: '{valor}'")
        return 0

df['salary'] = df['salary'].apply(codificar_salary)

# Verificar distribui√ß√£o ap√≥s codifica√ß√£o
logging.info("\nDistribui√ß√£o ap√≥s codifica√ß√£o:")
logging.info(df['salary'].value_counts())
logging.info("Percentual de cada classe:")
logging.info(df['salary'].value_counts(normalize=True) * 100)

# Verificar se h√° ambas as classes
if df['salary'].nunique() < 2:
    logging.warning("‚ö†Ô∏è ERRO: Apenas uma classe encontrada nos dados!")
    logging.warning(f"Valores √∫nicos em salary: {df['salary'].unique()}")
    
    # Investigar mais a fundo
    df_original = pd.read_csv('4-Carateristicas_salario.csv')
    logging.info("\nValores √∫nicos no arquivo original:")
    for valor in df_original['salary'].unique():
        logging.info(f"'{valor}' (aparece {(df_original['salary'] == valor).sum()} vezes)")
    
    # Tentar uma codifica√ß√£o diferente
    logging.info("\nTentando codifica√ß√£o alternativa...")
    df['salary'] = df_original['salary'].apply(lambda x: 1 if '>' in str(x) else 0)
    logging.info("Nova distribui√ß√£o:")
    logging.info(df['salary'].value_counts())

else:
    logging.info("‚úÖ Ambas as classes presentes nos dados")

# Separar X e y
X = df[numerical_features + categorical_features]
y = df['salary']

# Verificar valores ausentes
logging.info(f"\nValores ausentes em X: {X.isnull().sum().sum()}")
logging.info(f"Valores ausentes em y: {y.isnull().sum()}")

# Tratar valores '?' como NaN primeiro
X = X.replace('?', pd.NA)
logging.info(f"Valores ausentes ap√≥s tratar '?': {X.isnull().sum().sum()}")

# Remover registros com valores ausentes se necess√°rio
if X.isnull().sum().sum() > 0:
    logging.info("Removendo registros com valores ausentes...")
    mask = ~(X.isnull().any(axis=1) | y.isnull())
    X = X[mask]
    y = y[mask]
    logging.info(f"Registros restantes: {len(X)}")

# Verificar distribui√ß√£o final antes da divis√£o
logging.info(f"\nDistribui√ß√£o final de y antes da divis√£o:")
logging.info(y.value_counts())
logging.info(f"Classes √∫nicas: {y.unique()}")

# PARAR SE N√ÉO H√Å AMBAS AS CLASSES
if y.nunique() < 2:
    logging.error("\n‚ùå ERRO CR√çTICO: Imposs√≠vel continuar com apenas uma classe!")
    logging.error("Verifique os dados originais e a codifica√ß√£o da vari√°vel 'salary'")
    exit()

# Pr√©-processador para colunas (OneHot para categ√≥ricas, StandardScaler para num√©ricas)
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)
    ])

# Divis√£o treino/teste com stratify para manter propor√ß√£o das classes
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42, 
    stratify=y  # Importante: mant√©m propor√ß√£o das classes
)

# Verificar distribui√ß√£o nas parti√ß√µes
logging.info(f"\nDistribui√ß√£o em y_train:")
logging.info(y_train.value_counts())
logging.info(f"\nDistribui√ß√£o em y_test:")
logging.info(y_test.value_counts())

# Ajustar transforma√ß√µes
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Novo: Garantir que X_test_processed √© do tipo float
X_test_processed = X_test_processed.astype(float)

logging.info("\nPr√©-processamento conclu√≠do. Formato final dos dados:")
logging.info(f"X_train: {X_train_processed.shape}")
logging.info(f"X_test: {X_test_processed.shape}")
logging.info(f"y_train classes √∫nicas: {y_train.unique()}")
logging.info(f"y_test classes √∫nicas: {y_test.unique()}")

# ================================================
# 5. BALANCEAMENTO DOS DADOS (SMOTE) - CONDICIONAL
# ================================================
logging.info("\n" + "="*60)
logging.info("BALANCEAMENTO DOS DADOS")
logging.info("="*60)

# Verificar se precisa de balanceamento
class_counts = y_train.value_counts()
minority_ratio = min(class_counts) / max(class_counts)

logging.info(f"Propor√ß√£o da classe minorit√°ria: {minority_ratio:.2f}")

if minority_ratio < 0.3:  # Se a classe minorit√°ria representa menos de 30%
    logging.info("Dataset desbalanceado. Aplicando SMOTE...")
    from imblearn.over_sampling import SMOTE
    
    # Verificar se h√° pelo menos 2 classes antes do SMOTE
    if len(y_train.unique()) >= 2:
        sm = SMOTE(random_state=42)
        X_train_processed, y_train = sm.fit_resample(X_train_processed, y_train)
        
        logging.info("‚úÖ SMOTE aplicado com sucesso")
        logging.info("Distribui√ß√£o ap√≥s SMOTE:")
        logging.info(y_train.value_counts())
    else:
        logging.warning("‚ö†Ô∏è Imposs√≠vel aplicar SMOTE: apenas uma classe nos dados de treino")
else:
    logging.info("Dataset j√° balanceado. SMOTE n√£o necess√°rio.")

# ================================================
# 6. MODELAGEM PREDITIVA
# ================================================
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import joblib

# Fun√ß√£o para treinar e avaliar modelos
def avaliar_modelo(nome, modelo, X_train, y_train, X_test, y_test):
    """
    Treina e avalia um modelo de classifica√ß√£o.

    Par√¢metros:
    - nome: str, nome do modelo
    - modelo: inst√¢ncia do classificador
    - X_train, y_train: dados de treino
    - X_test, y_test: dados de teste

    Retorna:
    - modelo treinado ou None se erro ocorrer
    """
    # Verificar se h√° pelo menos 2 classes nos dados de treino
    if len(y_train.unique()) < 2:
        logging.warning(f"\n‚ö†Ô∏è {nome}: Imposs√≠vel treinar - apenas uma classe nos dados de treino")
        logging.warning(f"Classes em y_train: {y_train.unique()}")
        return None
    
    logging.info(f"üîÑ Iniciando treinamento: {nome}")
    try:
        modelo.fit(X_train, y_train)
        y_pred = modelo.predict(X_test)
        logging.info(f"\n{'='*20}\n{nome}\n{'='*20}")
        logging.info(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
        logging.info(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}")
        logging.info("\nClassification Report:")
        logging.info(classification_report(y_test, y_pred))
        logging.info("Matriz de Confus√£o:")
        logging.info(confusion_matrix(y_test, y_pred))
        # Avalia√ß√£o AUC-ROC
        from sklearn.metrics import roc_auc_score
        try:
            auc = roc_auc_score(y_test, y_pred)
            logging.info(f"AUC-ROC: {auc:.4f}")
        except ValueError as e:
            logging.warning(f"N√£o foi poss√≠vel calcular AUC-ROC: {e}")
        logging.info(f"‚úÖ Modelo {nome} treinado com sucesso")
        return modelo
    except Exception as e:
        logging.warning(f"\n‚ö†Ô∏è Erro ao treinar {nome}: {str(e)}")
        return None

# Inicializar modelos
modelos = {
    "Regress√£o Log√≠stica": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),
    "k-NN (k=5)": KNeighborsClassifier(n_neighbors=5)
}

# Avaliar todos os modelos e salvar
modelos_treinados = {}
for nome, modelo in modelos.items():
    modelo_treinado = avaliar_modelo(nome, modelo, X_train_processed, y_train, X_test_processed, y_test)
    if modelo_treinado is not None:
        modelos_treinados[nome] = modelo_treinado

logging.info("‚úÖ Todos os modelos treinados e avaliados")

# ================================================
# SALVAR MODELOS E PREPROCESSOR
# ================================================
logging.info("\n" + "="*60)
logging.info("SALVANDO MODELOS E PREPROCESSOR")
logging.info("="*60)

# Criar diret√≥rio models se n√£o existir
os.makedirs("models", exist_ok=True)

# Salvar o modelo Random Forest (melhor para interpretabilidade)
if "Random Forest" in modelos_treinados:
    joblib.dump(modelos_treinados["Random Forest"], "random_forest_model.joblib")
    logging.info("‚úÖ Modelo Random Forest salvo como 'random_forest_model.joblib'")

# Salvar o preprocessor
joblib.dump(preprocessor, "preprocessor.joblib")
logging.info("‚úÖ Preprocessor salvo como 'preprocessor.joblib'")

# Gerar nomes das features ap√≥s o preprocessamento
ohe = preprocessor.named_transformers_['cat']
cat_feature_names = ohe.get_feature_names_out(categorical_features)
feature_names = numerical_features + list(cat_feature_names)

# Salvar informa√ß√µes sobre as features
feature_info = {
    'numerical_features': numerical_features,
    'categorical_features': categorical_features,
    'feature_names': feature_names
}
joblib.dump(feature_info, "feature_info.joblib")
logging.info("‚úÖ Informa√ß√µes das features salvas como 'feature_info.joblib'")

# Salvar alguns dados de exemplo para testes
sample_data = X_test.head(5)
joblib.dump(sample_data, "sample_data.joblib")
logging.info("‚úÖ Dados de exemplo salvos como 'sample_data.joblib'")

logging.info(f"\nüìÅ Todos os arquivos salvos com sucesso!")
logging.info("Arquivos dispon√≠veis para o dashboard:")
logging.info("- random_forest_model.joblib")
logging.info("- preprocessor.joblib")
logging.info("- feature_info.joblib")
logging.info("- sample_data.joblib")

# ================================================
# 7. CLUSTERING E REGRAS DE ASSOCIA√á√ÉO
# ================================================
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd

# Usar PCA para reduzir a dimensionalidade para 2D para visualiza√ß√£o
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed)

# KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_pca)

# Plot dos clusters
plt.figure(figsize=(8, 6))
# Configurar fundo transparente
plt.gca().patch.set_alpha(0.0)
plt.gcf().patch.set_alpha(0.0)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', s=10)
plt.title("Visualiza√ß√£o dos Clusters com KMeans (PCA 2D)")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.tight_layout()
plt.savefig("imagens/kmeans_clusters.png", transparent=True, bbox_inches='tight')
plt.close()
logging.info("\nClustering com KMeans conclu√≠do. Gr√°fico salvo em 'imagens/kmeans_clusters.png'.")

# -------------------------------------
# Regras de associa√ß√£o (via Apriori)
# -------------------------------------
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Criar um dataset amostral com transa√ß√µes bin√°rias (apenas para demonstra√ß√£o simples)
amostras = df[['education', 'occupation', 'relationship', 'sex', 'salary']].astype(str).values.tolist()
te = TransactionEncoder()
transacoes = te.fit(amostras).transform(amostras)
df_transacoes = pd.DataFrame(transacoes, columns=te.columns_)

# Aplicar Apriori
freq_items = apriori(df_transacoes, min_support=0.05, use_colnames=True)
regras = association_rules(freq_items, metric="confidence", min_threshold=0.6)

# Mostrar as 5 principais regras
logging.info("\nTop 5 Regras de Associa√ß√£o:")
logging.info(regras[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())

# ================================================
# 8. INTERPRETA√á√ÉO DOS MODELOS
# ================================================
logging.info("\n" + "="*60)
logging.info("INTERPRETA√á√ÉO DOS MODELOS")
logging.info("="*60)

if "Random Forest" in modelos_treinados:
    # -------------------------------
    # FEATURE IMPORTANCE - Random Forest
    # -------------------------------
    importances = modelos_treinados["Random Forest"].feature_importances_
    
    # Obter nomes das colunas ap√≥s transforma√ß√£o
    ohe = preprocessor.named_transformers_['cat']
    cat_feature_names = ohe.get_feature_names_out(categorical_features)
    feature_names = numerical_features + list(cat_feature_names)
    
    # Combinar com os valores
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    
    # Mostrar top 15
    logging.info("\nTop 15 Features mais importantes (Random Forest):")
    logging.info(feature_importance_df.head(15))
    
    # Plot com fundo transparente
    plt.figure(figsize=(10, 6))
    # Configurar fundo transparente
    plt.gca().patch.set_alpha(0.0)
    plt.gcf().patch.set_alpha(0.0)
    
    sns.barplot(data=feature_importance_df.head(15), x='Importance', y='Feature')
    plt.title("Import√¢ncia das Features - Random Forest")
    plt.tight_layout()
    plt.savefig("imagens/feature_importance_rf.png", transparent=True, bbox_inches='tight')
    plt.close()
    
    # -------------------------------
    # COEFICIENTES - Regress√£o Log√≠stica (se dispon√≠vel)
    # -------------------------------
    if "Regress√£o Log√≠stica" in modelos_treinados:
        coef = modelos_treinados["Regress√£o Log√≠stica"].coef_[0]
        
        coef_df = pd.DataFrame({
            'Feature': feature_names,
            'Coefficient': coef
        }).sort_values(by='Coefficient', key=abs, ascending=False)
        
        logging.info("\nTop 15 Coeficientes mais importantes (Regress√£o Log√≠stica):")
        logging.info(coef_df.head(15))
        
        # Plot com fundo transparente
        plt.figure(figsize=(10, 6))
        # Configurar fundo transparente
        plt.gca().patch.set_alpha(0.0)
        plt.gcf().patch.set_alpha(0.0)
        
        sns.barplot(data=coef_df.head(15), x='Coefficient', y='Feature')
        plt.title("Coeficientes - Regress√£o Log√≠stica")
        plt.tight_layout()
        plt.savefig("imagens/coefficients_lr.png", transparent=True, bbox_inches='tight')
        plt.close()
    
    # -------------------------------
    # SHAP VALUES (corrigido)
    # -------------------------------
    try:
        import shap
        
        # Converter para array denso se necess√°rio e garantir tipo float
        X_train_shap = X_train_processed
        if hasattr(X_train_shap, 'toarray'):
            X_train_shap = X_train_shap.toarray()
        
        # Garantir que √© float64
        X_train_shap = X_train_shap.astype(np.float64)
        
        # Usar apenas uma amostra menor para SHAP (mais r√°pido)
        sample_size = min(1000, X_train_shap.shape[0])
        X_sample = X_train_shap[:sample_size]
        
        logging.info(f"\nGerando SHAP values para {sample_size} amostras...")
        
        explainer = shap.TreeExplainer(modelos_treinados["Random Forest"])
        shap_values = explainer.shap_values(X_sample)
        
        # Se o modelo retorna valores SHAP para cada classe, usar a classe positiva
        if isinstance(shap_values, list):
            shap_values_plot = shap_values[1]  # Classe positiva (>50K)
        else:
            shap_values_plot = shap_values
        
        # Summary plot com fundo transparente
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values_plot, X_sample, feature_names=feature_names, show=False)
        plt.tight_layout()
        plt.savefig("imagens/shap_summary_rf.png", transparent=True, bbox_inches='tight')
        plt.close()
        
        # Bar plot SHAP com fundo transparente
        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_values_plot, X_sample, feature_names=feature_names, plot_type="bar", show=False)
        plt.tight_layout()
        plt.savefig("imagens/shap_bar_rf.png", transparent=True, bbox_inches='tight')
        plt.close()
        
        logging.info("‚úÖ Gr√°ficos SHAP gerados com sucesso.")
        
    except ImportError:
        logging.warning("\n‚ö†Ô∏è Pacote SHAP n√£o instalado. Execute: pip install shap")
    except Exception as e:
        logging.warning(f"\n‚ö†Ô∏è Erro ao gerar gr√°ficos SHAP: {str(e)}")
        logging.warning("Continuando sem SHAP...")

else:
    logging.warning("‚ö†Ô∏è Modelo Random Forest n√£o foi treinado com sucesso.")

logging.info("‚úÖ Interpreta√ß√£o de modelos conclu√≠da")

logging.info("\nüéâ Pipeline completo executado!")
logging.info("\nüìÅ Arquivos gerados:")
logging.info("- Dados: 4-Carateristicas_salario.csv")
logging.info("- Modelo: random_forest_model.joblib")
logging.info("- Preprocessor: preprocessor.joblib")
logging.info("- Features: feature_info.joblib")
logging.info("- Amostras: sample_data.joblib")
logging.info("- Gr√°ficos: pasta imagens/ (todos com fundo transparente)")

# Sugest√£o final
logging.info("Sugest√£o: Para maior robustez, considere adicionar valida√ß√£o cruzada com cross_val_score.")
